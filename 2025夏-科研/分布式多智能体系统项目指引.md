# 分布式多智能体系统项目指引

## 杂项

住宿 通知

## 项目概述

和通义千问一起做一个极具挑战的开源项目---搭建一个全新的分布式多智能体系统
当前我和刘沛根同学做了架构设计，以及初步的系统搭建。
后续我们大家一起攻坚克难，做出一个分布式多智能体系统的标杆架构！
**7月7号启动，可以提前开始提前结束**

## 学习日程及资料

### 【1 在这里打卡即可】
https://www.yuque.com/bacon-8elzw/zzgtp1/vxvon1gbevdpegan?singleDoc#7MaR

| 学习任务                | 目标天数 | 实际天数     |
|---------------------|------|----------|
| **读论文（7.7-7.20）**   |      |          |
| agent综述             | 3    | 6.30-7.1 |
| 单agent              | 2    | 7.1      |
| 多agent              | 4    |          |
| agent社会模拟           | 5    |          |
| **学习与实践（7.21-8.3）** |      |          |
| 学习LangGraph         | 3    | 1        |
| 学习docker            | 2    | 1        |
| 了解k8s               | 2    |          |
| 环境部署与实践             | 12   |          |

### 【2 材料】
- **论文清单**: https://www.yuque.com/bacon-8elzw/zzgtp1/xonpn42nsmy7wuwt?singleDoc# 《论文清单》
- **实践手册**: https://www.yuque.com/bacon-8elzw/zzgtp1/cb2lxnspu0xsx7dm?singleDoc# 《实践手册》

## 论文清单

### agent入门学习课程：
- https://www.bilibili.com/video/BV1FCR3Y1EnJ/

李宏毅 agent 2025版    6.30 2h
首先强调一下AI领域的实效性 几个月前就是古代了
所以看arxiv 审稿几个月的国际会议更像经典回顾
原理：
agent定义上可以有无物理身体 
本质是对任务自行决策 多步互动灵活调整完成 不可测
范式过程：根据goal 不断执行action+observation循环 与环境交互

强化学习RL也属于这套范式 只是把goal换成max reward 但RL局限是每个任务都要训一遍且只服务一个任务
那么日益强大的LLM也可以结合人类goal自助进行决策交互循环
以国际象棋为例 目前依然无法进行合法顺利的对决 但依然可以做其他事
以上是将agent范式应用到LLM的思考
如果从LLM角度看agent 那么agent天然就是一种文字接龙任务 即goal+ observe1+act1 + observe2+act2... 并不需要LLM有改动

因此 LLM+agent只是并列于typical+agent的一种尝试
优势是无限的输出 无限的任务可能  
并且摒弃了typical方式下人为且无准确含义的reward参数设置 转而直接将环境文本输入

知名例子 2023.4斯坦福小镇 agent village
更大规模的实验有 MC游戏中的NPC
更现实的例子即用自然语言命令agent在PC上执行系列操作 甚至在2017年就有相关论文 当然当时没有GPT
还有用AI训练模型的任务 goal即strong baseline 过程即程序+正确率的循环 也有各种framework

更加通用真实的场景下
需要agent即时互动 即observe变化应该打断目前的action并立刻执行新的action
课程中提到的也即语音对话 间歇的无意义响应在语音对话中作为反馈也很重要

AI agent三块
- AI如何根据经验调整行为：memory与三大组件（读 写 改）
   feedback直接作为observe 不涉及任何调参
   问题是经验无限大 上下文有限长 所以加memory模块
   但是过长的上下文也是干扰 因此核心在于memory中的相关检索 read模块
   与RAG很类似 此时的memory是agent个人经历 非外部数据
   目前有streamBench用于评估大规模链式问题的正确率和调整能力
   同时会有一些结论：直接提应该做什么的memory比提供不能做什么memory要有效得多 单个prompt中也类似

   此外 memory的write也很重要 记忆并非全记录 大量鸡毛蒜皮记忆很影响检索 技巧是write模块也可以是一个agent 自行判断是否值得记录
   memory的第三个模组（无固定名字） reflect 反思 不但包括重组memory 也包括产生新的思考加入memory 相当于抽象+推理 也可以结构化成为知识图谱  同理也可以是其它agent甚至是agent自己
   关于reflect也有相关研究 如hippoRAG 类比人脑海马体
   目前chatGPT已经可以自定义个人memory并在对话中自动读写
   memory方面的研究从23年开始持续不断

- AI如何使用tool
   tool更像接口或者function 
   因此使用工具即function call
   不关心内部实现 只关心功能 如代码 联网 多模态
   目前更常用的形式是小模型偶尔调用大模型的能力

   tool的调用方式很多
   最通用的方法即用sys prompt告知：
   你可以把指令放在< tool >和</ tool >中间 把输出放在< output >和</ output >中间
   但本质上模型也只是输出包含上面格式的文本而已
   因此支持tool的模型应该在接受到上述调用文本时传入某个脚本 并抹去调用文本 只保留脚本输出作为模型回答
   个人在cursor使用过程中也会发现类似格式的泄露

   相关文章：例如语音输入本身就可以有一堆分析tool
   包括转文字 情绪分析 音色分析 口音 停顿等等

   工具说明作为sys prompt太多怎么办？依然通过memory和检索来解决
   模块即tooldescription ，make tools 和tools selection 
   而且LLM天生就是程序模型 因此可以自己产生tool并保存到memory

   tool的可信问题。正如LLM作为人类工具的不可靠  tool作为LLM的工具也不可靠
   依赖LLM初始训练时的判断 来决定tool的结果是否合理 如气温100摄氏度等问题
   一些好玩的文章 但不一定完全准确
   相关文章证明 外部知识（如数量级）与模型初始知识越吻合 模型就越相信外部知识 否则坚持自己的知识
   若外部知识本身存在冲突 那么模型更相信“AI味”的文章或知识
   若外部知识冲突且都是AI生成的 模型更相信时效性更高的文章 尤其注意文章来源并没有影响模型可信判断
   若文章内容完全一样但排版不一样 模型倾向于选择排版好看的文章

   最后 尽管tool的返回是对的 模型也可能输出错误结果
   且使用tool不一定总是高效的 要不要调用tool的判断也不一定高效


- AI能否能做计划 
   即planning的问题 plan是action的集合
   而agent是否针对goal并没有进行plan 
   只是在文本接龙
   很多文章强制在开始规定plan 但事实是plan就是根据环境动态变化的

   因此动态plan可以随着每步observe后跟着 引导模型调整
   但这是理想情况 很可能agent只是在训练或memory中照本宣科进行模仿 并非真的plan

   也有planBench来评估模型 如堆积木任务 反现实的一些逻辑任务 travelPlan任务

   总之目前的结论是有plan能力但不多

   强化方式是强制在每步action树形分支+提前剪枝 得到可能的路径
   问题是有些操作无法回溯
   因而应当将所有action分支放在假设中进行
   但是假设中又无法获取真实环境的observe
   所以需要自我判断
   类似脑内剧场 类似思考reasoning的能力
   然后如何避免模型想太多 做太少 也是研究方向





### agent综述论文（6篇）：
- https://github.com/HuggingAGI/AwesomeAgentPapers

仓库综述：
Agents与Agentic Workflow的本质区别
核心差异在于系统架构的自主权分配，直接决定了任务执行的灵活性与适用边界。用戏剧比喻：
Workflow是严格遵循剧本的演员
Agent是自带导演思维的即兴表演者
Agent需要构建认知架构
（如MetaGPT中的角色分工、AutoGPT的目标分解），
通常结合记忆机制（VectorDB）、工具库（Toolkit）与强化学习
实现持续进化。

1. 2024.01 Agent AI: Surveying the Horizons of Multimodal Interaction
重点是论文开头的图 从理论数据基建到应用 从物理世界、虚拟世界维度到范式（Paradigm）、具体产品的总结图
斯坦福大学、微软研究院、加州大学洛杉矶分校、华盛顿大学等

abstract
多模态AI系统可能会成为我们日常生活中的普遍存在。一个有前景的方法是将这些系统体现为物理和虚拟环境中的代理。目前，系统利用现有的基础模型作为构建交互式代理的基本构建块。将代理嵌入到这样的环境中可以增强模型处理和解释视觉和上下文数据的能力，这对于创建更复杂和上下文感知的AI系统至关重要。例如，一个能够感知用户行为、人类行为、环境物体、音频表达和场景集体情感的系统可以用来通知和指导代理在给定环境中的响应。为了加速具身多模态智能的研究，我们定义了“Agent AI”作为一种交互式系统类，这些系统能够感知视觉刺激、语言输入以及其他基于环境的数据，并能产生有意义的具身行动。特别是，我们探索了通过整合外部知识、多感官输入和人类反馈来改进基于下一步具身行动预测的代理系统。我们主张，在具身环境中开发具身AI系统也可以减少大型基础模型的幻觉倾向及其产生环境不正确输出的倾向。新兴的Agent AI领域涵盖了多模态交互的更广泛的具身和代理方面。除了代理在物理世界中的行动和交互之外，我们还设想了一个未来，人们可以轻松地创建任何虚拟现实或模拟场景，并与其中体现的代理进行交互。

1 Introduction
1.1 Motivation：本文探讨了多模态交互的Agent AI新兴范式，追溯到1956年达特茅斯会议对AI的定义，强调了回归AI基础的重要性，并突出了Agent AI在游戏、机器人和医疗等领域的变革潜力。
1.2 Background：介绍了支持Agent AI概念的相关研究论文，包括大型基础模型（LLMs和VLMs）、具身AI和交互式学习。讨论了LLMs在任务规划和机器人研究中的应用。
1.3 Overview：概述了文章的结构，聚焦于多模态Agent AI（MAA）在游戏、机器人和医疗领域的应用，旨在提供关于Agent AI研究的全面知识。
2 Agent AI Integration
2.1 Infinite AI Agent：讨论了AI代理的能力，包括预测性建模、决策、处理模糊性以及持续改进。介绍了无限代理的概念，即能够将记忆信息转移到新领域或场景中。
2.2 Agent AI with Large Foundation Models：探讨了大型基础模型在Agent AI中的应用，包括幻觉、偏差与包容性、数据隐私与使用、可解释性与解释性、推理增强和法规。
2.3 Agent AI for Emergent Abilities：探讨了交互式代理AI系统中的泛化挑战，介绍了混合现实与知识推理交互的概念，以促进代理与人类在现实世界环境中的协作。
3 Agent AI Paradigm
3.1 LLMs and VLMs：讨论了如何利用LLMs和VLMs来引导Agent AI系统的组件，强调了LLMs在任务规划和VLMs在视觉编码方面的能力。
3.2 Agent Transformer Definition：提出了一种统一的代理变换器模型，该模型可以接受视觉、语言和特定于代理的标记作为输入。
3.3 Agent Transformer Creation：概述了创建代理变换器模型的过程，包括在领域内定义目标和通过监控与反馈进行持续改进。
4 Agent AI Learning
4.1 Strategy and Mechanism：讨论了训练Agent AI系统的不同策略和机制，包括强化学习、模仿学习、传统RGB方法、上下文学习和代理系统中的优化。
4.2 Agent Systems (zero-shot and few-shot level)：介绍了Agent AI模块和基础设施，强调了开发高效和有效的代理系统的需求。
4.3 Agentic Foundation Models (pretraining and finetune level)：探讨了预训练基础模型在Agent AI中的应用，强调了它们在多样化用例中的适用性。
5 Agent AI Categorization
5.1 Generalist Agent Areas：讨论了能够在多种情境和模态中与用户交互的通用代理。
5.2 Embodied Agents：探索了设计用于与物理世界交互的具身代理，包括行动代理和交互式代理。
5.3 Simulation and Environments Agents：讨论了在模拟环境中学习和交互的代理，这对于训练和测试AI系统至关重要。
5.4 Generative Agents：探讨了能够在游戏和混合现实设置中生成内容的生成性代理。
5.5 Knowledge and Logical Inference Agents：讨论了整合知识和逻辑推理以解决智能特定方面的代理。
5.6 LLMs and VLMs Agent：探讨了作为任务规划和推理代理的LLMs和VLMs在各个领域的应用。
6 Agent AI Application Tasks
6.1 Agents for Gaming：讨论了Agent AI在游戏中的应用，包括NPC行为、人与NPC的交互、基于代理的游戏分析和游戏场景合成。
6.2 Robotics：探索了Agent AI在机器人技术中的应用，重点在于视觉运动控制、基于语言的操控和技能优化。
6.3 Healthcare：讨论了Agent AI在医疗保健中的潜力，包括诊断代理、知识检索代理和远程医疗与监测。
6.4 Multimodal Agents：探讨了多模态代理在图像-语言理解和生成、视频和语言理解和生成以及多模态代理中的能力。
6.5 Video-language Experiments：介绍了将预训练的图像LLMs转换为视频理解的实验，强调了当前方法的潜力和局限性。
6.6 Agent for NLP：讨论了LLM代理、通用LLM代理、遵循指令的LLM代理以及NLP任务中的实验和结果。
7 Agent AI Across Modalities, Domains, and Realities
7.1 Agents for Cross-modal Understanding：讨论了跨模态理解的挑战，以及视觉编码器和LLMs联合调整的必要性。
7.2 Agents for Cross-domain Understanding：探讨了创建能够在不同领域操作的通用代理的挑战。
7.3 Interactive agent for cross-modality and cross-reality：讨论了开发能够在不同现实之间理解和执行任务的AI代理的发展。
7.4 Sim to Real Transfer：解决了在模拟中训练的模型在现实世界中表现不佳的“模拟到现实”问题，讨论了领域随机化、领域适应和改进模拟现实的技术。
8 Continuous and Self-improvement for Agent AI
8.1 Human-based Interaction Data：讨论了使用人类交互数据改进AI代理的方法，包括额外的训练数据、人类偏好学习和安全训练。
8.2 Foundation Model Generated Data：探讨了从基础模型生成训练数据的方法，包括指令调优方法和视觉-语言对。
9 Agent Dataset and Leaderboard
9.1 “CuisineWorld” Dataset for Multi-agent Gaming：介绍了用于多智能体游戏的CuisineWorld数据集，旨在评估多智能体系统的协作效率。
9.2 Audio-Video-Language Pre-training Dataset：介绍了VideoAnalytica基准测试，重点是对视频演示进行复杂推理任务的评估。
10 Broader Impact Statement
讨论了Agent AI在各个行业中的潜在积极影响，以及开发解决方案以应对现实世界问题的重要性。
11 Ethical Considerations
强调了Agent AI的伦理考虑，包括负责任的AI指南、透明度和最小化现实世界场景中的风险。
12 Diversity Statement
强调了多模态和具身AI中挑战和解决方案的多样性，旨在通过探索各种主题和观点来构建一个多样化的社区。

```note
挺大而全的  但是由于是2024年01的依然有点老
```

2. 2024.01 The Rise and Potential of Large Language Model Based Agents: A Survey
核心是启发式综述和一些讨论

由复旦NLP和米哈游公司共同撰写，从AI Agent的历史出发，全面梳理基于大型语言模型的智能代理现状
包括LLM-based Agent的背景、构成、应用场景以及备受关注的代理社会。
同时，探讨了Agent相关的前瞻开放问题，对相关领域的未来发展趋势具有重要价值。

长期以来，人类一直在追求等同于或超越人类水平的人工智能（AI），而Agent则被认为是大有可为的工具。人工智能代理是能够感知环境、做出决策和采取行动的人工实体。人们为开发智能代理做出了许多努力，但主要集中在算法或训练策略的改进上，以提高特定能力或特定任务的性能。实际上，社会各界缺乏的是一个通用而强大的模型，作为设计能适应各种场景的人工智能代理的起点。由于大型语言模型（LLM）所展示的多功能性，它们被视为人工通用智能（AGI）的潜在火花，为构建通用人工智能代理带来了希望。许多研究人员利用 LLM 作为构建人工智能代理的基础，并取得了重大进展。在本文中，我们将对基于 LLM 的代理进行全面研究。

我们首先追溯了代理概念从哲学起源到在人工智能中的发展，并解释了为什么 LLM 适合作为代理的基础。

在此基础上，我们提出了基于 LLM 的代理的一般框架，包括三个主要部分：大脑、感知和行动。

随后，我们从三个方面探讨了基于 LLM 的代理的广泛应用：单代理场景、多代理场景和人-代理合作。

随后，我们深入探讨了代理社会，探索了基于 LLM 的代理的行为和个性、代理社会中出现的社会现象以及它们对人类社会的启示。最后，我们讨论了这一领域的几个关键议题和悬而未决的问题。

1.1 早在 18 世纪，哲学家丹尼斯·狄德罗提出“如果一只鹦鹉能回答所有问题，它就可以被认为是聪明的”

代理涵盖了五个层次，描绘了从自然语言处理到通用人工智能的研究进展
**即语料库、互联网、感知、具身和社会**
纯语言模型建立在第二层次，使用互联网规模的文本输入和输出
如果赋予agent扩展的感知空间和行动空间，它们有可能达到第三和第四层次
还可以通过合作或竞争来处理更复杂的任务，可能实现第五个层次。

2.1 人工智能代理的起源
从哲学角度探讨代理的概念起源，其核心是具有行动能力的实体。在人工智能领域，代理被视为能够感知、决策和响应的人工实体。
2.2 智能体研究中的技术趋势
总结了人工智能代理发展的几个阶段，包括符号代理、反应型代理、基于强化学习的智能体、具有迁移学习和元学习的智能体，以及基于大型语言模型的智能体，分析了各阶段的特点和局限性。
2.3 为什么大型语言模型适合作为智能体大脑的主要组成部分
阐述了LLM具备自主性、反应性、主动性和社会能力等关键属性，使其非常适合担任人工智能代理的主要组件或控制器。
3 智能体的诞生：基于大型语言模型的智能体构建
收到agent定义启发，我们提出了基于大型语言模型的代理人的一般概念框架，
包括三个关键部分：**大脑（brain）、感知（perception）、行动（action）**，该框架可以根据不同的应用进行定制

3.1 大脑主要由LLM组成，负责信息处理、决策等；大脑是人工智能代理人的核心，因为它不仅存储关键的记忆、信息和知识，还承担信息处理、决策、推理和规划等基本任务。它是影响代理人是否能表现出智能行为的关键决定因素。

3.2 感知模块扩展代理的感知空间；感知空间从仅文本扩展到包括文本、声音、视觉、触觉、嗅觉等多种感官模式的多模态空间

3.3 行动模块扩展代理的行动空间。环境交互。具备文本输出、进行具身行动和使用工具，以便更好地响应环境变化并提供反馈，甚至改变和塑造环境。

4 实践中的智能体: 利用人工智能造福人类
探讨了基于LLM的代理在
4.1 单代理场景  讨论它们在基于文本的任务和模拟探索环境中的表现，强调它们在处理特定任务、推动创新以及展示类人生存技能和适应能力方面的能力
4.2 多代理场景  多代理系统应用中，代理之间的互动方式，这些代理进行合作、谈判或竞争。无论互动模式如何，代理共同努力朝着共享目标前进
4.3 人-代理合作中  隐私安全、伦理限制和数据不足等方面的潜在局限性

5 代理社会：从个体性到社会性
研究了基于LLM的代理的行为和个性、代理社会中出现的社会现象以及它们对人类社会的启示，设想了由AI代理组成的社会场景。
5.1 这些代理是否表现出类人行为、具有人格特征

6 Discussion
6.1 LLM研究与代理研究之间的互惠互利
讨论了LLM研究和代理研究如何相互促进，推动彼此领域的发展。
6.2 基于LLM代理的评估
从效用、社交性、价值观和持续演变能力四个维度探讨了对基于LLM的代理的评估方法。
6.3 安全性、可信度以及基于大型语言模型代理的其他潜在风险
分析了基于LLM的代理存在的一些风险，如对抗鲁棒性、可信性、误用、失业威胁等，并讨论了相应的缓解策略。
6.4 扩大代理数量
讨论了增加代理数量的潜在优势和挑战，以及预先确定和动态扩展的方法。
6.5 开放问题
探讨了基于LLM的代理是否代表通用人工智能的潜在路径、从虚拟模拟环境到物理环境的挑战、人工智能代理的集体智能、服务型代理等未解决的问题。
7 结论
论文全面系统地概述了基于LLM的智能体领域，从哲学起源到技术发展，从框架构建到应用探索，再到社会影响和未来展望，旨在为相关领域的研究人员和从业人员提供灵感和参考。
```note
比上述课程更宏观的理论 输入的模态拓展  输出的物理拓展  最终形成一个agent社会   至于框架本质无殊 还是一个核心处理模块+输入+输出
```

3. 2023.08 A survey on large language model based autonomous agents

由中国人民大学高瓴人工智能学院研究团队撰写。系统综述了基于大语言模型（LLM）的自主Agent研究进展。

传统自主代理依赖有限知识在封闭环境中训练，与人类学习模式存在显著差异，而LLM通过海量网络知识获取展现出类人智能潜力。

论文从构建、应用与评估三方面展开分析：在构建层面，提出统一框架整合记忆、规划与工具使用等核心模块，通过外部知识库和工具调用增强LLM的决策能力。

abstract
先前的研究关注于具有有限知识的智能体在隔绝环境下的训练，这与人类学习方式大相径庭，因此很难像人类一样做决策。最近，LLMs通过获取的丰富web知识，在完成人类等级只能上展现出非凡的潜力。本文对该领域研究进行了全面的调查，作者首先讨论了基于LLMs的自主智能体的构建，提出一个大一统的框架，接着作者阐述了基于LLMs的自主智能体在社科、自然科学和工程领域的各种应用，最后作者深入研究了基于LLMs的自主智能体常用的评估策略。基于这些研究，作者也提出了这个领域的一些挑战和未来方向。
1.1 Introduction
自主智能体一直是实现通用人工智能（AGI）的有前途的方法。然而，先前的研究通常基于隔绝环境和简单启发式策略，这限制了它们的决策能力。最近，大型语言模型（LLMs）通过获取丰富的网络知识，在展现类人类智能方面取得了突破。基于LLMs的自主智能体能够利用其类人类能力来完成任务，这推动了学术界和工业界对这一领域的研究兴趣。本文通过构建、应用和评估三个方面，对基于LLMs的自主智能体进行了全面的综述。
2.2 LLM-based Autonomous Agent Construction
基于LLM的自主智能体需要合理的架构设计来最大化其能力。文章提出一个**统一框架，包括画像模块、记忆模块、规划模块和动作模块。** 画像模块定义智能体的角色，记忆模块帮助智能体积累经验，规划模块赋予智能体分解任务的能力，动作模块将智能体的决策付诸实践。此外，智能体的能力获取策略分为基于微调和无需微调两类。
3.3 LLM-based Autonomous Agent Application
基于LLM的智能体在多个领域展现出巨大潜力。在社会科学领域，它们可以用于心理学模拟、政治学与经济学研究、社交模拟、法律决策支持和研究助手等。在自然科学领域，它们适用于文档与数据管理、实验助手和自然科学教育。在工程领域，它们在土木工程、计算机科学与软件工程、工业自动化以及机器人与人工智能等方面具有应用价值。
4.4 LLM-based Autonomous Agent Evaluation
评估基于LLM的自主智能体有两种常见策略：**主观评估和客观评估。** 主观评估包括人类标注和图灵测试，适用于没有评估数据集或难以量化的场景。客观评估采用定量指标，包括任务成功指标、人类相似度度量和效率指标，以及评估协议和基准。
5.5 Related Surveys
尽管大语言模型领域已有大量综述，但专门针对基于LLM的自主智能体的综述尚属首次。本文整理了100多篇相关著作，涵盖构建、应用和评估。
6.6 Challenges
基于LLM的自主智能体面临多个挑战，包括角色扮演能力、泛化人类对齐、提示工程的鲁棒性、幻觉问题、知识边界和效率问题。
```note
文章更老2023 感觉不够宏观 记忆啦规划啦动作啦 完全分属思考核心和输出模块  但是画像挺有意思  本质上是规定这个思考核心的目标 即课程里的goal
```

4. 2024.02 Large Language Model based Multi-Agents: A Survey of Progress and Challenges
重点是一张树形发展图 一些baseline和dataset

本文由南方科技大学等联合撰写。分析了LLM多智能体所模拟的领域和环境、这些智能体的特征及其通信方式，以及促进智能体能力发展的机制。

还总结了该领域常用的数据集和基准，以便有兴趣的研究人员能够方便地获取。为读者提供关于LLM多智能体系统研究进展和挑战的深刻见解。

摘要
本文深入探讨了基于大型语言模型（LLM）的多智能体系统，重点研究了其在复杂问题解决和世界模拟方面的进展。文章分析了这些系统所模拟的领域和环境，智能体的描述和通信方式，以及促进智能体能力增长的机制。此外，文章还总结了常用的数据集和基准。

1 Introduction
单智能体发展：基于LLM的智能体在理解和生成人类指令、促进复杂交互和决策方面迅速发展。
多智能体系统优势：通过集体智能和多个智能体的专门配置文件和技能，解决更复杂的任务。
LLM能力：交流能力和广泛知识，专注于特定任务的潜力。
应用领域：软件开发、多机器人系统、社会模拟、政策模拟、游戏模拟等。
2 Background
2.1 单智能体系统
决策思维：基于LLM的智能体在提示引导下分解复杂任务，进行多路径探索和学习。
工具使用：利用外部工具和资源增强功能。
记忆力：上下文学习和长期内存，保持上下文一致性。
2.2 单智能体与多智能体系统对比
单代理：关注内部机制和与外部环境的相互作用。
多代理：强调不同的主体概况、主体间的互动和集体决策过程。
3 剖析LLM-MA系统
3.1 代理与环境接口
沙盒环境：模拟或虚拟环境，代理可以自由交互。
物理环境：真实世界环境，代理与物理实体交互。
无环境：代理不与任何环境交互，专注于通信。
3.2 代理个人资料
预定义：系统设计人员显式定义。
模型生成：按模型创建，如大型语言模型。
数据派生：基于现有数据集构建。
3.3 代理通信
通信范式：协作、辩论、竞争。
通信结构：分层、分散、集中、消息池。
通信内容：通常采用文本形式。
3.4 代理能力获取
反馈：来自环境、代理交互、人类反馈。
自我调节：记忆、自我进化、动态生成。
4 应用
4.1 问题解决
软件开发：模拟不同角色，协作解决复杂挑战。
具身代理：多个机器人协作执行物理任务。
科学实验：多智能体协作进行科学实验，人类监督关键。
科学辩论：智能体辩论增强集体推理能力。
4.2 世界模拟
社会模拟：模拟社会行为，探索社会动态。
游戏模拟：测试博弈论假设，模拟人类互动。
心理学：直接应用心理学实验或模拟社会行为。
经济模拟：模拟经济和金融环境中的行为。
推荐系统：模拟用户和商品交互，传播偏好。
政策制定：模拟政策制定及其潜在影响。
疾病传播模拟：模拟疾病传播和人类反应。
5 实施工具和资源
5.1 多智能体框架
MetaGPT：将人类工作流程嵌入语言模型代理。
CAMEL：促进代理之间的自主合作。
AutoGen：通用框架，允许使用语言模型创建应用程序。
5.2 数据集和基准
问题解决场景：评估多智能体合作或辩论的规划和推理能力。
世界模拟场景：评估模拟世界与现实世界的一致性或分析代理行为。
结论
本文系统总结了基于LLM的多智能体系统的研究，包括其基本方面和面临的挑战。文章强调了这些系统在复杂问题解决和世界模拟方面的潜力，并提供了常用的数据集和基准以供进一步研究。

```note
启发式列举了可能的应用领域  研究要用的基准和data
```

5. 2024.05 Personal LLM Agents: Insights and Survey about the Capability, Efficiency and Security

本篇论文由清华大学人工智能产业研究院联合多家公司撰写。聚焦于个人大型语言模型（LLM）智能体，探讨了其架构、能力、效率和安全性等关键问题。

通过对领域专家的意见收集和分析，总结了个人LLM代理的关键组件和设计选择，并深入讨论了实现智能、高效和安全的个人LLM代理所面临的挑战，以及相应的解决方案。

正文
前言
研究背景：科幻小说中的智能个人助理逐渐成为现实，但现有IPA在灵活性和可扩展性方面仍有限。LLM的出现为IPA发展带来新机遇，可显著增强其能力和实用性。
目前存在的问题：现有IPA在任务执行、上下文感知和记忆方面存在不足，且难以支持大规模任务。
目前已有的解决方案：LLM能够使IPA自主解决复杂问题，提供更智能的服务。
研究目标：探讨个性化LLM代理的路线图、设计选择、主要挑战和可能的解决方案，重点关注“个性化”部分，包括个人数据的管理和利用、个人资源的使用、在个人设备上的部署以及提供个性化服务。
智能个人助理(IPA)的简史
IPA历史的时间轴视图：分为四个阶段，从早期的语音识别技术到现代智能手机上的虚拟助理服务，再到LLM时代的智能聊天机器人和基于LLM的IPA软件。
IPA的技术观点：以任务自动化能力为重点，介绍四种主要类型的技术：基于模板设计的技术、基于监督学习的技术、基于强化学习的技术和早期采用基础模型的技术。
个性化LLM代理：定义和见解
定义：将个性化LLM代理定义为与个人数据、个人设备和个人服务深度集成的基于LLM的智能体，旨在帮助用户减少重复和繁琐的工作，更多地专注于有趣和重要的事务。
分析方案：通过调查领域专家的见解，总结个性化LLM智能体的关键组成部分、智力水平分类法和专家对常见问题的意见。
个性化LLM代理——基础的能力
任务执行：讨论了任务执行的自动化方法、代理框架和评估方法。
上下文感知：探讨了上下文感知的感知源和感知目标，以及如何通过硬件传感器、软件传感器和多传感器组合实现上下文感知。
记忆力：分析了记忆力的获取、管理和利用，包括日志记录、记忆推理以及原始数据管理、记忆增强的LLM推理和智能体自进化。
个性化LLM代理——效率
推理：讨论了模型压缩技术、推理加速技术、内存压缩技术和能耗优化技术，以提高LLM推理的效率。
定制：探讨了提升上下文加载效率和LLM微调效率的方法，包括参数高效的微调技术、高效的优化器设计和训练数据管理。
记忆操作：关注高效的外部内存检索，包括提升搜索效率和优化工作流效率。
安全和隐私
保密性：讨论了本地数据处理、同态加密、数据屏蔽和信息流控制等方法，以保护用户隐私。
诚实性：分析了对抗攻击、后门攻击和提示注入攻击等新型攻击形式，以及相应的防御方法。
可靠性：从问题、改进和检测三个角度讨论了LLM的可靠性，包括幻觉、未识别的操作和连续可靠性问题，以及对齐、自我反省和检索增强等改进方法，还有验证、解释和中间特征分析等检测方法。
结论与展望
研究现状：目前关于个性化LLM代理的研究还处于早期阶段，任务执行能力相对不足，支持的功能范围较窄，且在效率、可靠性和可用性方面存在许多关键问题。
未来展望：需要共同努力为个性化LLM代理建立整个软件/硬件堆栈和生态系统，同时确保其良性和辅助性质。

```note
聚焦个人agent的问题 突出的是个性化
论文展望的是个性化智能体将成为 toC 终端的主要软件范式。
探讨核心在于架构、能力、效率和安全性
```

6. 2024.02 Understanding the planning of LLM agents: A survey

本篇论文由中国科学技术大学和华为诺亚方舟实验室的团队撰写。

主要梳理了LLM-based Agent 中的规划（planning）能力。

对基于大型语言模型（LLM）的智能体规划进行了系统性回顾，提出了一个关于 LLM-Agent 规划的现有工作的分类体系，
将其分为五个方向：**任务分解、计划选择、外部辅助模块、反思和记忆**
每个方向都进行了全面分析，并讨论了该研究领域的挑战。作者旨在提供一个关于基于 LLM 的代理规划能力的系统性视角，这在该领域尚属首次。论文还评估了在四个基准测试上的几种代表性方法，为未来的研究方向提供了见解。

1.1 引言 (Introduction)
自主智能代理：被定义为能够完成特定任务的智能实体。它们通过感知环境、规划和执行动作来实现目标。
规划的重要性：规划是代理最关键的能力之一，它要求代理进行复杂的理解、推理和决策过程。
规划任务的一般表述：在给定的时间步 ，环境表示为 ，动作空间为 ，任务目标为 ，以及在时间步 的动作为 ，规划过程可以被表达为生成一系列动作的序列： 。其中， 和 分别代表LLM的参数和任务的提示。
传统方法的局限性：以往的工作主要依赖于符号方法或基于强化学习的方法，如规划领域定义语言（PDDL）或策略学习。这些传统方法有其局限性，例如符号方法需要将自然语言描述的问题转换为符号建模，这可能需要人类专家的努力，而且缺乏容错性。强化学习方法通常需要与环境的大量样本（交互）来学习有效策略，这在数据收集耗时或成本高昂的场景中可能不切实际。
LLM的潜力：近年来，大型语言模型（LLM）的出现标志着一个范式的转变。LLM在多个领域取得了显著的成功，展示了在推理、工具使用、规划和指令跟随方面的重要智能。这种智能为将LLM作为代理的认知核心提供了可能性，从而有潜力提高规划能力。
本文工作：尽管已有调查尝试总结LLM的技术，但文献中往往缺乏对规划能力的详细分析。本调查旨在分析最新的研究工作，讨论优势和局限性，并提供对基于LLM的代理规划能力的系统性视角。


2.2 任务分解 (Task Decomposition)

现实世界中的任务通常是复杂和多步骤的，直接通过单步规划过程来解决复杂任务是一项巨大挑战。任务分解通过将复杂任务分解为多个简单子任务，使得规划过程更加可行。

分解方法分类：任务分解方法主要分为两类：
分解优先方法（Decomposition-First Methods）：首先将任务分解为子目标，然后依次为每个子目标制定计划。
交错分解方法（Interleaved Decomposition Methods）：在任务分解和子任务规划之间进行交错，每次只揭示当前状态的一个或两个子任务。

分解优先方法的代表工作：
HuggingGPT：LLM作为控制器，负责将人类输入的任务分解为子任务，选择模型，并生成最终响应。
Plan-and-Solve：将原始的“让我们逐步思考”转变为两步提示指令：“我们首先制定计划”和“我们执行计划”。
ProgPrompt：将自然语言描述的任务转化为编码问题，将每个动作形式化为函数，每个对象表示为变量。
交错分解方法的代表工作：
Chain-of-Thought (CoT)：通过构建的轨迹指导LLM对复杂问题进行推理，利用LLM的推理能力进行任务分解。
Zero-shot CoT：使用“让我们逐步思考”的指令，解锁LLM的零样本推理能力。
ReAct：将推理和规划解耦，交替进行推理（思考步骤）和规划（行动步骤）。
讨论：
分解优先方法的优势在于创建了子任务与原始任务之间的强关联，降低了任务遗忘和幻觉的风险。但需要额外的调整机制，以避免某个步骤的错误导致整体失败。
交错分解方法可以根据环境反馈动态调整分解，提高了容错性。但对于复杂任务，过长的轨迹可能导致LLM产生幻觉，偏离原始目标。
挑战：尽管任务分解显著提高了LLM代理解决复杂任务的能力，但仍存在挑战，包括任务分解引入的额外开销、时间成本和计算成本，以及LLM的上下文长度限制。
3.3 多计划选择 (Multi-Plan Selection)

由于任务的复杂性和LLM固有的不确定性，对于给定任务，LLM代理可能会生成多种不同的计划。多计划生成涉及利用生成模型解码过程中的不确定性，通过不同的采样策略来产生多个候选计划。 - Self-consistency：采用简单的直觉，即复杂问题的解决方案很少是唯一的。通过温度采样、top-k采样等策略，获得多个不同的推理路径。 - Tree-of-Thought (ToT)：提出“采样”和“提议”两种策略来生成计划。LLM在解码过程中会采样多个计划，并通过少量示例提示生成各种计划。 - Graph-of-Thought (GoT)：在ToT的基础上增加了思想的转换，支持任意思想的聚合。 - LLM-MCTS 和 RAP：利用LLM作为启发式策略函数，通过蒙特卡洛树搜索（MCTS）算法来获取多个潜在动作。

最优计划选择：在候选计划中选择最优计划时，采用了多种启发式搜索算法。
Self-consistency：使用简单的多数投票策略，将得票最多的计划视为最优选择。
Tree-of-Thought (ToT)：支持树搜索算法，如广度优先搜索（BFS）和深度优先搜索（DFS），使用LLM评估多个动作并选择最优动作。
LLM-MCTS 和 RAP：也使用树结构辅助多计划搜索，但它们采用MCTS算法进行搜索。
LLM A：利用经典的A算法协助LLM搜索，使用当前位置到目标位置的切比雪夫距离作为启发式成本函数来选择最优路径。
讨论：
多计划选择的可扩展性显著优势在于提供了在广阔搜索空间中更广泛探索潜在解决方案的能力。
然而，这种优势伴随着计算需求的增加，尤其是对于具有大量token计数或计算的模型，这在资源受限的情况下尤为重要。
LLM在计划评估中的作用引入了新的挑战，因为LLM在任务排名方面的表现仍在审查中，需要进一步验证和微调其在此特定情境下的能力。
LLM的随机性质为选择过程增加了随机性，可能影响所选计划的一致性和可靠性。

4.4 外部规划器辅助规划 (External Planner-Aided Planning)

尽管LLM在推理和任务分解方面展现出了强大的能力，但在面对具有复杂约束的环境时，例如数学问题求解或生成可执行动作，仍然面临挑战。

方法分类：根据引入的规划器类型，这些方法可以分为两类：
符号规划器（Symbolic Planner）：基于形式化模型，如PDDL，使用符号推理来找到从初始状态到目标状态的最优路径。
神经规划器（Neural Planner）：通过强化学习或模仿学习技术训练的深度模型，针对特定领域展现出有效的规划能力。
符号规划器的代表工作：
LLM+P：通过结合基于PDDL的符号规划器，使用LLM将问题组织成PDDL语言格式，并利用Fast Downward solver进行规划。
LLM-DP：特别为动态交互环境设计，将环境反馈信息形式化为PDDL语言，并使用BFS solver生成计划。
LLM+PDDL：在LLM生成的PDDL模型中增加手动验证步骤，并提出使用LLM生成的计划作为局部搜索规划器的初始启发式解。
LLM+ASP：将自然语言描述的任务转换为ASP问题，然后使用ASP求解器CLINGO生成计划。
神经规划器的代表工作：
CALM：结合了语言模型和基于RL的神经规划器，使用语言模型生成候选动作，然后通过DRRN策略网络重新排序以选择最优动作。
SwiftSage：将规划过程分为快速思考和慢速思考，快速思考通过模仿学习训练的DT模型实现，慢速思考则涉及LLM基于当前状态的推理和规划。
讨论：
在这些策略中，LLM主要扮演支持角色，其主要功能包括解析文本反馈并提供额外的推理信息以协助规划，特别是在解决复杂问题时。
传统的符号AI系统在构建符号模型时复杂且依赖于人类专家，而LLM可以加速这一过程，有助于更快更优地建立符号模型。
符号系统的优势包括理论完备性、稳定性和可解释性。将统计AI与LLM结合，有望成为未来人工智能发展的主要趋势。


5.5 反思和精炼 (Reflection and Refinement)
反思和精炼是规划过程中不可或缺的组成部分，它们增强了LLM代理规划的容错能力和错误纠正能力。由于LLM在规划过程中可能产生幻觉或在复杂问题上推理能力不足，导致错误或陷入“思维循环”，反思和总结失败有助于代理纠正错误并在后续尝试中打破循环。

Self-refine： 利用迭代过程，包括生成、反馈和精炼。在每次生成后，LLM为计划产生反馈，促进基于反馈的调整。
Reflexion： 扩展了ReAct方法，通过引入评估器来评估轨迹。LLM在检测到错误时生成自我反思，帮助纠正错误。
CRITIC： 使用外部工具，如知识库和搜索引擎，来验证LLM生成的动作。然后利用外部知识进行自我纠正，显著减少事实错误。
InteRecAgent： 使用称为ReChain的自我纠正机制。LLM用于评估由交互推荐代理生成的响应和工具使用计划，总结错误反馈，并决定是否重新规划。
LEMA： 首先收集错误的规划样本，然后使用更强大的GPT-4进行纠正。这些纠正后的样本随后用于微调LLM代理，从而在各种规模的LLaMA模型上实现显著的性能提升。
讨论： - 自我反思策略类似于强化学习的原则，其中代理作为决策者，环境反馈触发策略网络的更新。然而，与深度强化学习通过修改模型参数来更新不同，在LLM代理中，这种更新是通过LLM自身的自我反思来实现的，最终形成文本形式的反馈。 - 这些文本反馈可以作为长期和短期记忆，通过提示影响代理后续的规划输出。然而，目前还没有确凿的证据证明这种文本形式的更新最终能够使LLM代理达到特定目标。

6.6 记忆增强规划 (Memory-Augmented Planning)
记忆是提升代理规划能力的关键途径，可以帮助代理从经验中学习并适应新的情境。 - RAG-based Memory（基于RAG的记忆）： - 概念：使用检索增强生成（Retrieval-Augmented Generation, RAG）技术，将记忆以文本形式存储，并在需要时检索出来辅助规划。 - 方法：如MemoryBank、TiM 和 RecMind，这些方法通过文本编码模型将记忆编码为向量，并建立索引结构，以便在规划时检索与当前任务相关的经验。 - Embodied Memory（体现记忆）： - 概念：通过微调（fine-tuning）LLM，将代理的历史经验样本嵌入到模型参数中，从而增强记忆能力。 - 方法：如CALM 和 TDT，这些方法使用从代理与环境交互中收集的数据来微调模型，使其能够记住与规划相关的信息，并在规划任务中表现更好。 - 记忆更新方式： - RAG-based：提供了实时、低成本的外部记忆更新，但依赖于检索算法的准确性。 - Finetuning：提供了更大的记忆容量，但记忆更新成本较高，并且在保留细节方面存在挑战。 - 讨论： - 记忆增强的LLM代理在规划中表现出更强的增长潜力和容错能力，但记忆生成在很大程度上依赖于LLM自身的生成能力。 - 通过自我生成的记忆来提升较弱LLM代理的能力仍然是一个具有挑战性的领域。 - 挑战：尽管记忆增强LLM代理在规划方面表现出优势，但它们在记忆生成上仍然面临挑战，特别是在自我生成记忆方面。

7.7 评估 (Evaluation)
评估代理的规划能力是研究领域中的一个关键问题。作者调查了几种主流的基准测试方法，并将它们分为以下几类：

交互式游戏环境（Interactive Gaming Environments）：
提供基于代理动作的实时多模态反馈，如文本和视觉反馈。
例如Minecraft，代理需要收集材料制作工具以获得更多奖励，常用评价指标是代理创建的工具数量。
基于文本的交互环境（Text-based interactive environments）：
代理位于用自然语言描述的环境中，动作和位置有限。
常用评价指标是成功率或获得的奖励，例如ALFWorld和ScienceWorld。
交互式检索环境（Interactive Retrieval Environments）：
模拟人类在现实生活信息检索和推理的过程。
代理可以与搜索引擎和其他网络服务交互，通过搜索关键词或执行点击、前进、后退操作来获取更多信息，完成问答任务或信息检索任务。
交互式编程环境（Interactive Programming Environments）：
模拟程序员与计算机之间的交互，测试代理解决计算机相关问题的规划能力。
代理需要与计算机交互，通过编写代码或指令来解决问题。

实验：
作者在四个基准测试上进行了实验，以验证代表性方法的性能。这些基准测试包括ALFWorld、ScienceWorld、HotPotQA和FEVER，涵盖了交互式游戏和问答基准测试。
实验结果显示，性能随着成本的增加而提高，表明更详细的思考（即消耗更多的token）可以带来性能上的提升。
另外，对于复杂任务，示例（例如Zero-shot CoT和Few-shot CoT）对于LLM进一步理解任务至关重要。
反思（Reﬂexion）在提高成功率方面发挥了关键作用，尤其是在复杂任务中，显示了LLM具备错误纠正能力。
讨论：
现有的基准测试大多依赖于任务的最终完成状态，缺乏细粒度的逐步评估。
环境反馈通常是规则化的、简化的，并且与现实世界场景有距离。
未来的发展方向可能包括利用高智能模型如LLM来设计更现实的评估环境。

8.8 结论和未来方向 (Conclusions and Future Directions)
进展总结：自LLM展现出智能以来，使用LLM增强代理规划能力的研究受到了越来越多的关注。作者概述了主要的研究方向，并在前文中对各种方法进行了详细比较和分析。
实验结果：作者在四个基准测试上进行了实验，比较了几种代表性方法的有效性，并指出随着投入成本的增加，性能也随之提高。
挑战：尽管这些研究在规划能力上取得了增强，但仍存在一些重大挑战：
幻觉问题（Hallucinations）：LLM在规划过程中可能会产生幻觉，导致非理性的计划或无法遵循复杂指令。
生成计划的可行性：与基于符号的人工智能相比，LLM在优化过程中可能难以遵守复杂约束，导致生成的计划缺乏可行性。
计划的效率：现有LLM代理的规划过程可能没有考虑生成计划的效率，未来的发展可能需要引入额外的效率评估模块。
未来方向：
多模态环境反馈：考虑集成多模态大型模型的发展，并重新审视相关的规划策略，以处理包括图像、音频等在内的多模态反馈。
细粒度评估：利用高智能模型如LLM设计更现实的评估环境，提供更细致的逐步评估，以更好地模拟现实世界场景。


```note
聚焦于agent的plan能力

plan分解方法有分解优先方法（创建了子任务与原始任务之间的强关联，但需要额外调整机制避免错误传播）
               交错分解方法（可根据环境反馈动态调整，提高容错性，但轨迹过长导致幻觉）
多plan选择 多计划生成、最优计划选择（多种启发式搜索算法选择最优计划

外部辅助plan 符号规划器（基于形式化模型） 神经规划器（用强化学习或模仿学习）

反思和精炼 类比RL 但是更新通过LLM自我反思实现，文本反馈可作为记忆影响后续规划，效果需进一步验证。

memory增强的plan  有RAG（实时、低成本更新，依赖检索算法）和 微调LLM即embedding（容量大但是微调成本高）

评估的话 整体是投入越多效果越好
交互式游戏环境：如Minecraft，评价指标为代理创建的工具数量。
基于文本的交互环境：如ALFWorld、ScienceWorld，评价指标为成功率或获得的奖励。
交互式检索环境：模拟信息检索和推理过程，评价指标为任务完成情况。
交互式编程环境：测试代理解决计算机问题的规划能力。

挑战：幻觉问题、生成计划的可行性、计划的效率。
未来方向：多模态环境反馈、细粒度评估。
```



### 单agent论文（3篇）：
1. **HuggingGPT**
   - 论文链接：https://arxiv.org/pdf/2303.17580
   - Github：https://github.com/microsoft/JARVIS

2023.03 HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face
微软亚洲研究院和浙江大学

核心：以非常工程的prompt控制 通过单一chatGPT调用丰富的hugging face模型，处理多模态复杂任务。
有意思的是 项目仓库名称就是钢铁侠的“贾维斯”

论文认为解决大型语言模型（LLMs）局限性，是迈向人工通用智能（AGI）的关键一步。
当前LLMs在处理复杂信息（如视觉和语音）、多子任务协作以及特定任务表现上存在不足。
HuggingGPT通过将AI模型功能以语言形式表示，使ChatGPT能够像大脑一样管理和调用这些模型。
其工作流程分为四个阶段：**任务规划、模型选择、任务执行和响应生成。**
目前，HuggingGPT已在HuggingFace上集成了数百个模型，涵盖24个任务类型。
实验表明，HuggingGPT能够处理多模态信息和复杂任务。
该项目已开源，但需要OpenAI API支持。

具体来说
步骤1：任务规划（Task Planning）
Prompt设计：综合使用“基于规范的指令”和“基于示范的解析”的方法。
基于规范的指令：对Task做了规范约束，必须是包含任务内容（task）、任务id（id）、任务依赖（dep）和参数（args）等四个要素的格式：[{"task": task, "id", task_id, "dep": dependency_task_ids, "args": {"text": text, "image": URL, "audio": URL, "video": URL}}]。
基于示范的解析：Prompt中提供若干“示例”（Demonstrations），激发大模型的基于上下文学习或者Few-shot learning能力，从而产生更好的理解和生成。
提供的信息：Prompt还会给LLM提供Available Task List和Chat Logs，即可用的HuggingFace模型列表和对话历史，提供更多、更准确的上下文信息。
步骤2：模型选择（Model Selection）
Prompt设计：比较直白，但同样给LLM提供了明确的“备选模型”（Candidate Models）的信息。
备选模型数据获取：
模型描述获取：从HuggingFace网站获取每个模型的功能、架构、领域、许可等各种信息，作为喂给LLM做模型筛选的输入。
模型初筛：由于Prompt有长度限制，根据步骤1的结果中的task字段，和模型描述中的功能或分类做匹配，得到该任务类型下的所有模型描述。
模型细筛：按照任务初筛之后，模型还是会很多，于是还要进行进一步的细筛。文章中提到是根据模型的下载量取TopN来实现，下载量很大程度上反应了该模型的质量和受欢迎程度，可以作为进一步筛选的标准。
步骤3：任务执行（Task Execution）
任务执行就是唤起选中的模型，执行算法，最后返回结果给到ChatGPT。这个过程不是由LLM执行的，从而不需要Prompt设计。
步骤4：响应生成（Response Generation）
Prompt设计：汇总各个模型的运行结果，给用户生成一个答案。Prompt需要描述整个过程和结果，先直接回答用户请求，然后描述任务过程和展示模型推理结果。

```note
核心是工程化的prompt写法 作为参考  论文也较取巧和古早
```

2. **KG-Agent**
   - 论文链接：https://arxiv.org/pdf/2402.11163

2024.02 KG-Agent: An Efficient Autonomous Agent Framework for Complex Reasoning over Knowledge Graph

核心是一个框架 用于提升LLM对知识图谱（KG）的推理能力
使其更好完成知识密集型任务

背景
LLM 在多跳推理和知识密集型任务上能力有限，而 KG 中存储着大量结构化知识三元组，但 LLM 难以有效利用其信息，现有基于 LLM 的 KG 推理方法如检索增强和协同增强存在诸多局限性。
技术方案
设计目标 ：打造自主推理方法，让智能体在推理时可主动决策无需人工协助；使较小模型（如 7B LLM）能有效执行复杂推理，不依赖闭源 LLM APIs。
技术贡献 ：
策划多功能工具箱，扩展 LLM 处理结构化数据的能力，设计了提取工具、逻辑工具、语义工具三种类型的工具，分别用于从 KG 中提取信息、对信息进行基本操作及拓展对 KG 的操作。
利用 KG 推理数据集合成基于代码的指令数据来微调 LLM，先是根据 KG 推理链生成程序，再合成指令数据。
提出基于工具选择和存储器更新的自主迭代机制，集成调谐的 LLM、多功能工具箱、基于 KG 的执行器和知识存储器，实现自主推理。
KG - Agent 架构
KG 工具箱 ：涵盖上述三类工具，共 12 个工具，可满足 KG 推理中的多种操作需求。
KG - Agent 指令微调 ：用现有 KGQA 数据集生成 KG 推理流程，进而合成指令数据集，用于微调轻量 LLM（LLaMA2 - 7B）。
KG 自主推理 ：包含 LLM、多功能工具箱、基于 KG 的执行器、知识记忆存储器四个组件。知识记忆存储器记录上下文和有用信息，工具选择规划器基于知识记忆选工具，记忆更新执行器执行工具后更新知识记忆，然后迭代进行工具选择和记忆更新，直至得到答案实体停止。
实验结果
域内任务 ：仅用 10K 样本调整较小的 LLM，在 WebQSP、CWQ、GrailQA 等数据集上，相比基于 LLM 的基线（使用更多原始训练集）获 F1 相对改善，如约用 36% 和 23% 原始训练集量，在 CWQ 和 GrailQA 上分别获 7.5% 和 2.7% F1 相对改善；与传统基于子图的推理方法比，LM 生成的 SPARQL 查询获更完整答案集且结构化查询能支持复杂操作。
域外任务 ：zero-shot 性能优于全数据监督微调模型，如在 WQ - Freebase 和 TQ - Wiki 上分别相对提高 9.7% 和 8.5% 精度；相比小型预训练语言模型，可在 zero - shot 设置中利用外部 KG 取得一致改进。
总结与展望
KG - Agent 是首个协同 LLM 和 KG 执行 KG 复杂推理的自主智能体框架，仅用较少调优样本和相对小的 LLM，性能优于多个基线。未来计划扩展框架以处理更多结构化数据类型，如数据库和表格等。

```note
一个用来处理KG推理的agent框架  个人要补一下KG的知识
```


3. **ReadAgent**
   - 论文链接：https://arxiv.org/pdf/2402.09727
   - demo：https://read-agent.github.io/

2024.02 A Human-Inspired Reading Agent with Gist Memory of Very Long Contexts
核心是长上下文或记忆的检索方式

针对大语言模型在处理长文档时面临的上下文长度限制问题，
提出了一种创新的人类启发式阅读代理（ReadAgent）框架。
该研究旨在借鉴人类交互式阅读长文档的自然方式，
通过**智能分段、要点记忆和交互式查找**，
显著扩展有效上下文窗口长度，提升长文档阅读理解性能。

一、研究背景与目标
大语言模型在处理长文档时，受限于上下文长度，输出变长往往导致性能下降。针对这一问题，ReadAgent通过模仿人类阅读行为，将长文本内容分块处理，并压缩关键信息以形成要点记忆，同时结合交互式查找策略，实现了对长文档的有效理解与回答。
二、ReadAgent核心流程
情节分页：将长文本内容智能分块，通过提示LLM（大语言模型）在阅读连续文本时自动识别断点，将两个断点之间的内容作为一个页面进行处理。这种分页方式既保留了文本的连贯性，又降低了单次处理的复杂度。
记忆要点：对每一页进行信息压缩，提示LLM对每一页进行总结，将确切内容缩短为要点，并将要点与上下文相关联。这种要点记忆方式既保留了文本的关键信息，又大大减少了存储和处理的开销。
交互式查找：结合要点和原文进行作答。LLM在上下文查找信息时，除了使用要点记忆进行快速定位外，还可以在原始文本中查找相关细节。ReadAgent提供了两种查找策略：一次并行查找所有页面（ReadAgent-P）和一次顺序查找一页（ReadAgent-S）。前者适合于快速获取多页面信息，后者则更适合于逐页精细阅读。
三、实验结果与分析
实验采用了QuALITY、NarrativeQA和QMSum三个长文档阅读理解测试集，基座模型选择了PaLM 2-L（上下文8K）和GPT-3.5。与基线方案RAG（检索增强生成）相比，ReadAgent在性能上实现了显著提升。评测指标包括LR-1（完全匹配百分比）和LR-2（完全匹配和部分匹配百分比），同时引入LLM作为裁判进行一致性判断。结果显示，ReadAgent在三个任务上均优于基线方案，且将有效上下文窗口扩展了3-20倍。
四、结论与启示
ReadAgent通过长文本分段存储和摘要技术，结合交互检索策略，在保持良好性能的同时显著减少了计算开销。该方案在两个方面为知识治理提供了自动化思路：情节分页和记忆要点可用于RAG方案的知识库优化；逐页答案生成则可直接用于减少计算资源消耗。这些启示对于提升大语言模型在长文档处理领域的应用性能具有重要价值。
综上所述，《A Human-Inspired Reading Agent with Gist Memory of Very Long Contexts》论文提出了一种创新的长文档阅读理解框架ReadAgent，通过智能分段、要点记忆和交互式查找技术，实现了对长文档的有效处理与理解。该研究不仅拓展了大语言模型在长文档处理领域的应用边界，也为知识治理和智能问答系统提供了新的解决方案。
```note
agent中memory方面的长效理解处理技术工作
```






### 多agent论文（5篇）：
1. **MetaGPT**
   - 论文链接：https://arxiv.org/pdf/2308.00352.pdf
   - GIthub：https://github.com/geekan/MetaGPT

2023.08 MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework

核心是多agent协作编程的framework

借用了meta-learning的learning-to-learn来生成框架，旨在生成结构化的输出，包括高质量的需求文档、设计图、流程图和具体的实现接口等。

MetaGPT为了能够实现上面的目的，需要建立在多个标准下构建的Agent，使得Agent能够按照相应的标准进行自主地完成问题分析、系统设计、代码生成、更新、执行和调试等工作。

Agent in SOPs
对于一个比较复杂的工程任务（例如让大模型写一个flappy bird游戏），需要通过扮演不同角色的大模型之间相互协同来完成这个庞大的任务。每个角色的大模型完成具体的一个子环节的工作
分别是
plan、
requirement analysis、
architectural design、
sys design、
coding、
test、
acceptance check（验收）

定义好各个Agent的角色后，需要设置工作流程（Work Flow）：

产品经理角色Agent：根据用户的产品需求进行深入分析，并将任务进行分解，形成结构化的需求文档（Product Requirements Document，PRD）；（preliminary functional breakdown）；
架构师角色Agent：接收到产品经理角色大模型生成的PRD，将其转换为具体的系统设计、数据结构设计、文件列表等；
项目管理角色Agent：将架构师设计的系统等待开发工作进行任务分解（具体的代码接口功能）；
软件工程师角色Agent：接收到具体的接口和功能，进行代码生成，包括类、方法等；
QA工程师角色Agent：对于实现好的功能函数和类进行测试。
最后交付给用户，由用户进行评价反馈。

核心协作方式：
一种类似多轮对话的形式进行的，但不同于传统的文本对话，这里多个Agent之间对话采用结构化的文档（documents）或图表（diagrams）作为对话内容。
共享消息池：按照流程，上一个Agent通常只会跟下一个Agent进行交流，然而下一个Agent可能无法获取到上一个Agent之前的Agent的信息。因此设置一个共享消息池，每一个Agent可以将其产生的结构化消息存入到该池中，也可以通过信息检索策略从中提取消息。
Self-reflection：代码生成可能会存在幻觉，因此借助执行器和Debug工具来不断地完善代码。

实验：HumanEval：包含164个人工编写的程序任务；
MBPP：包含427个Python程序任务；
SoftwareDev：本文提出的benchmark，包含70个经典的软件开发任务

不足：MetaGPT中的Agent角色是固定的，且Action空间和顺序也是固定的，即每个流程要执行的Action都是事先定义好的，工具的使用也是在固定的Action之后完成。因此大模型在思考和生成的整个流程是被固定的。
```note
感觉是古早时期 人类流程设计大于agent规划的一个框架尝试
```

2. **AutoAgents**
   - 论文链接：https://arxiv.org/pdf/2309.17288.pdf
   - Github：https://github.com/Link-AGI/AutoAgents



3. **Large Language Model based Multi-Agents: A Survey of Progress and Challenges**
   - 论文链接：https://arxiv.org/pdf/2402.01680.pdf



4. **AutoGen**
   - 论文链接：https://arxiv.org/pdf/2308.08155
   - Github：https://github.com/microsoft/autogen



5. **Agent Hospital**
   - 论文链接：https://arxiv.org/pdf/2405.02957

    




### 多agent社会模拟论文（7篇）：
1. **斯坦福小镇（*重点）**
   - 论文链接：https://arxiv.org/abs/2304.03442 
   - 讲解视频：（1）https://b23.tv/CmDaFvh （2）https://b23.tv/hWCaQGh
   - GitHub: https://github.com/Turing-Project/SimAIWorld?tab=readme-ov-file

2. **Agentsociety （*重点）**
   - 论文链接：https://arxiv.org/abs/2502.08691 
   - 学习手册：https://agentsociety.readthedocs.io/en/latest/index.html
   - Github: https://github.com/tsinghua-fib-lab/AgentSociety

3. **A Survey on Social Simulation**
   - 论文连接：https://arxiv.org/abs/2412.03563

4. **OASIS**
   - 论文连接：https://arxiv.org/pdf/2411.11581
   - Github：https://github.com/camel-ai/oasis

5. **YuLan-OneSim**
   - 论文链接：https://arxiv.org/pdf/2505.07581
   - Github：https://github.com/RUC-GSAI/YuLan-OneSim

6. **LMAgent**
   - 论文连接：https://arxiv.org/pdf/2412.09237

7. **Light Society**
   - 论文连接：https://arxiv.org/pdf/2506.12078













## 实践手册

### Multi-Agent框架：

#### LangGraph
基于LangChain打造的Multi-Agent框架，通过引入有向循环图的理念，打造了一个极具灵活性和可定制性的解决方案。LangGraph不仅适用于各类Multi-Agent任务，还能支持几乎所有的多智能体编排应用，使其成为那些面临复杂任务、追求高度灵活性和定制化能力的开发者的首选工具。

- 使用教程：https://langchain-ai.github.io/langgraph/#get-started
- GitHub：https://github.com/langchain-ai/langgraph



#### docker

**docker学习资料（可选）**

课程视频（可以学习一些操作命令）：
- https://www.bilibili.com/video/BV1Zn4y1X7AZ/?share_source=copy_web

**安装docker**
- 操作文档：https://help.aliyun.com/zh/ecs/use-cases/install-and-use-docker#8dca4cfa3dn0e

**docker 国内镜像配置**
- Linux操作文档：https://blog.csdn.net/Lichen0196/article/details/137355517
- MacOS操作文档：https://blog.csdn.net/u011308433/article/details/132058397

**vscode 连接docker 容器**
- 操作文档：https://blog.csdn.net/qq_19716143/article/details/132310200

**docker 命令**

镜像操作：
- 检索：`docker search <镜像名>`
- 下载：`docker pull <镜像名>`
- 列表：`docker images`
- 删除：`docker rmi <镜像名:版本/镜像ID>`

容器命令：
- 运行：`docker run <容器名/ID>`
- 查看：`docker ps`
- 停止：`docker stop <容器名/ID>`
- 启动：`docker start <容器名/ID>`
- 重启：`docker restart <容器名/ID>`
- 状态：`docker stats <容器名/ID>`
- 日志：`docker logs <容器名/ID>`
- 进入：`docker exec -it <容器ID> /bin/bash`
- 删除：`docker rm (-f 强制删除运行中的) <容器名/ID>`
- 停止所有运行的容器：`docker stop $(docker ps -q)`
- 删除所有容器：`docker rm $(docker ps -a -q)`

**docker run 详解（docker run <参数> <容器名/ID>)**
- `-d` 在后台运行
- `--name` 容器名字
- `-p （本机端口号）:（容器端口号）` 端口映射

**保存镜像 （从容器中）**
- 提交：`docker commit`
- 保存：`docker save`
- 加载：`docker load`

**分享社区**
- 登录：`docker login`
- 命名：`docker tag`
- 推送：`docker push`

**构建镜像 （从dockerfile中）**
```bash
docker build [OPTIONS] <构建上下文路径>
```

常用选项：
- `-t <镜像名:标签>`：指定镜像名称和标签（如 -t my-app:v1）。
- `-f <Dockerfile路径>`：指定非默认名称的 Dockerfile（如 -f ./path/to/Dockerfile）。

构建上下文路径：
通常是包含 Dockerfile 的目录（如 . 表示当前目录）。
Docker 会将此目录下的文件发送给守护进程（需注意排除无关文件，通过 .dockerignore 配置）。

**管理多容器（与k8s类似，可在单机上开发使用）**
```bash
# 启动所有容器服务
docker compose -f <yaml文件路径> up -d (后台启动)

# 停止并删除所有容器、网络
docker compose -f <yaml文件路径> down 
-v：同时删除匿名卷  --rmi all：删除所有相关镜像

# 修改 Dockerfile 后需要完全停止并删除现有容器，以确保新镜像被正确应用
docker compose down

# 使用修改后的 Dockerfile 重新构建镜像，并启动新的容器
docker compose up -d --build
```


#### k8s
- 课程视频：https://www.bilibili.com/video/BV1Se411r7vY/?share_source=copy_web&vd_source=c54b306489f493ab850de370eaa4b0c5
- 通过kubectl工具连接集群：https://help.aliyun.com/zh/ack/ack-managed-and-ack-dedicated/user-guide/obtain-the-kubeconfig-file-of-a-cluster-and-use-kubectl-to-connect-to-the-cluster?spm=a2c4g.11186623.0.0.613ad176I3BtlO#section-2za-tyw-p71 