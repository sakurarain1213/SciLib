# 分布式多智能体系统项目指引

## 杂项

住宿 通知

## 项目概述

和通义千问一起做一个极具挑战的开源项目---搭建一个全新的分布式多智能体系统
当前我和刘沛根同学做了架构设计，以及初步的系统搭建。
后续我们大家一起攻坚克难，做出一个分布式多智能体系统的标杆架构！
**7月7号启动，可以提前开始提前结束**

## 学习日程及资料

### 【1 在这里打卡即可】
https://www.yuque.com/bacon-8elzw/zzgtp1/vxvon1gbevdpegan?singleDoc#7MaR

| 学习任务                | 目标天数 | 实际天数     |
|---------------------|------|----------|
| **读论文（7.7-7.20）**   |      |          |
| agent综述             | 3    | 6.30-7.1 |
| 单agent              | 2    | 7.1      |
| 多agent              | 4    | 7.9-7.10 |
| agent社会模拟           | 5    | 7.18     |
| **学习与实践（7.21-8.3）** |      |          |
| 学习LangGraph         | 3    | 1        |
| 学习docker            | 2    | 1        |
| 了解k8s               | 2    |          |
| 环境部署与实践             | 12   |          |

### 【2 材料】
- **论文清单**: https://www.yuque.com/bacon-8elzw/zzgtp1/xonpn42nsmy7wuwt?singleDoc# 《论文清单》
- **实践手册**: https://www.yuque.com/bacon-8elzw/zzgtp1/cb2lxnspu0xsx7dm?singleDoc# 《实践手册》

## 论文清单

### agent入门学习课程：
- https://www.bilibili.com/video/BV1FCR3Y1EnJ/

李宏毅 agent 2025版    6.30 2h
首先强调一下AI领域的实效性 几个月前就是古代了
所以看arxiv 审稿几个月的国际会议更像经典回顾
原理：
agent定义上可以有无物理身体 
本质是对任务自行决策 多步互动灵活调整完成 不可测
范式过程：根据goal 不断执行action+observation循环 与环境交互

强化学习RL也属于这套范式 只是把goal换成max reward 但RL局限是每个任务都要训一遍且只服务一个任务
那么日益强大的LLM也可以结合人类goal自助进行决策交互循环
以国际象棋为例 目前依然无法进行合法顺利的对决 但依然可以做其他事
以上是将agent范式应用到LLM的思考
如果从LLM角度看agent 那么agent天然就是一种文字接龙任务 即goal+ observe1+act1 + observe2+act2... 并不需要LLM有改动

因此 LLM+agent只是并列于typical+agent的一种尝试
优势是无限的输出 无限的任务可能  
并且摒弃了typical方式下人为且无准确含义的reward参数设置 转而直接将环境文本输入

知名例子 2023.4斯坦福小镇 agent village
更大规模的实验有 MC游戏中的NPC
更现实的例子即用自然语言命令agent在PC上执行系列操作 甚至在2017年就有相关论文 当然当时没有GPT
还有用AI训练模型的任务 goal即strong baseline 过程即程序+正确率的循环 也有各种framework

更加通用真实的场景下
需要agent即时互动 即observe变化应该打断目前的action并立刻执行新的action
课程中提到的也即语音对话 间歇的无意义响应在语音对话中作为反馈也很重要

AI agent三块
- AI如何根据经验调整行为：memory与三大组件（读 写 改）
   feedback直接作为observe 不涉及任何调参
   问题是经验无限大 上下文有限长 所以加memory模块
   但是过长的上下文也是干扰 因此核心在于memory中的相关检索 read模块
   与RAG很类似 此时的memory是agent个人经历 非外部数据
   目前有streamBench用于评估大规模链式问题的正确率和调整能力
   同时会有一些结论：直接提应该做什么的memory比提供不能做什么memory要有效得多 单个prompt中也类似

   此外 memory的write也很重要 记忆并非全记录 大量鸡毛蒜皮记忆很影响检索 技巧是write模块也可以是一个agent 自行判断是否值得记录
   memory的第三个模组（无固定名字） reflect 反思 不但包括重组memory 也包括产生新的思考加入memory 相当于抽象+推理 也可以结构化成为知识图谱  同理也可以是其它agent甚至是agent自己
   关于reflect也有相关研究 如hippoRAG 类比人脑海马体
   目前chatGPT已经可以自定义个人memory并在对话中自动读写
   memory方面的研究从23年开始持续不断

- AI如何使用tool
   tool更像接口或者function 
   因此使用工具即function call
   不关心内部实现 只关心功能 如代码 联网 多模态
   目前更常用的形式是小模型偶尔调用大模型的能力

   tool的调用方式很多
   最通用的方法即用sys prompt告知：
   你可以把指令放在< tool >和</ tool >中间 把输出放在< output >和</ output >中间
   但本质上模型也只是输出包含上面格式的文本而已
   因此支持tool的模型应该在接受到上述调用文本时传入某个脚本 并抹去调用文本 只保留脚本输出作为模型回答
   个人在cursor使用过程中也会发现类似格式的泄露

   相关文章：例如语音输入本身就可以有一堆分析tool
   包括转文字 情绪分析 音色分析 口音 停顿等等

   工具说明作为sys prompt太多怎么办？依然通过memory和检索来解决
   模块即tooldescription ，make tools 和tools selection 
   而且LLM天生就是程序模型 因此可以自己产生tool并保存到memory

   tool的可信问题。正如LLM作为人类工具的不可靠  tool作为LLM的工具也不可靠
   依赖LLM初始训练时的判断 来决定tool的结果是否合理 如气温100摄氏度等问题
   一些好玩的文章 但不一定完全准确
   相关文章证明 外部知识（如数量级）与模型初始知识越吻合 模型就越相信外部知识 否则坚持自己的知识
   若外部知识本身存在冲突 那么模型更相信“AI味”的文章或知识
   若外部知识冲突且都是AI生成的 模型更相信时效性更高的文章 尤其注意文章来源并没有影响模型可信判断
   若文章内容完全一样但排版不一样 模型倾向于选择排版好看的文章

   最后 尽管tool的返回是对的 模型也可能输出错误结果
   且使用tool不一定总是高效的 要不要调用tool的判断也不一定高效


- AI能否能做计划 
   即planning的问题 plan是action的集合
   而agent是否针对goal并没有进行plan 
   只是在文本接龙
   很多文章强制在开始规定plan 但事实是plan就是根据环境动态变化的

   因此动态plan可以随着每步observe后跟着 引导模型调整
   但这是理想情况 很可能agent只是在训练或memory中照本宣科进行模仿 并非真的plan

   也有planBench来评估模型 如堆积木任务 反现实的一些逻辑任务 travelPlan任务

   总之目前的结论是有plan能力但不多

   强化方式是强制在每步action树形分支+提前剪枝 得到可能的路径
   问题是有些操作无法回溯
   因而应当将所有action分支放在假设中进行
   但是假设中又无法获取真实环境的observe
   所以需要自我判断
   类似脑内剧场 类似思考reasoning的能力
   然后如何避免模型想太多 做太少 也是研究方向





### agent综述论文（6篇）：
- https://github.com/HuggingAGI/AwesomeAgentPapers

仓库综述：
Agents与Agentic Workflow的本质区别
核心差异在于系统架构的自主权分配，直接决定了任务执行的灵活性与适用边界。用戏剧比喻：
Workflow是严格遵循剧本的演员
Agent是自带导演思维的即兴表演者
Agent需要构建认知架构
（如MetaGPT中的角色分工、AutoGPT的目标分解），
通常结合记忆机制（VectorDB）、工具库（Toolkit）与强化学习
实现持续进化。

1. 2024.01 Agent AI: Surveying the Horizons of Multimodal Interaction
重点是论文开头的图 从理论数据基建到应用 从物理世界、虚拟世界维度到范式（Paradigm）、具体产品的总结图
斯坦福大学、微软研究院、加州大学洛杉矶分校、华盛顿大学等

abstract
多模态AI系统可能会成为我们日常生活中的普遍存在。一个有前景的方法是将这些系统体现为物理和虚拟环境中的代理。目前，系统利用现有的基础模型作为构建交互式代理的基本构建块。将代理嵌入到这样的环境中可以增强模型处理和解释视觉和上下文数据的能力，这对于创建更复杂和上下文感知的AI系统至关重要。例如，一个能够感知用户行为、人类行为、环境物体、音频表达和场景集体情感的系统可以用来通知和指导代理在给定环境中的响应。为了加速具身多模态智能的研究，我们定义了“Agent AI”作为一种交互式系统类，这些系统能够感知视觉刺激、语言输入以及其他基于环境的数据，并能产生有意义的具身行动。特别是，我们探索了通过整合外部知识、多感官输入和人类反馈来改进基于下一步具身行动预测的代理系统。我们主张，在具身环境中开发具身AI系统也可以减少大型基础模型的幻觉倾向及其产生环境不正确输出的倾向。新兴的Agent AI领域涵盖了多模态交互的更广泛的具身和代理方面。除了代理在物理世界中的行动和交互之外，我们还设想了一个未来，人们可以轻松地创建任何虚拟现实或模拟场景，并与其中体现的代理进行交互。

1 Introduction
1.1 Motivation：本文探讨了多模态交互的Agent AI新兴范式，追溯到1956年达特茅斯会议对AI的定义，强调了回归AI基础的重要性，并突出了Agent AI在游戏、机器人和医疗等领域的变革潜力。
1.2 Background：介绍了支持Agent AI概念的相关研究论文，包括大型基础模型（LLMs和VLMs）、具身AI和交互式学习。讨论了LLMs在任务规划和机器人研究中的应用。
1.3 Overview：概述了文章的结构，聚焦于多模态Agent AI（MAA）在游戏、机器人和医疗领域的应用，旨在提供关于Agent AI研究的全面知识。
2 Agent AI Integration
2.1 Infinite AI Agent：讨论了AI代理的能力，包括预测性建模、决策、处理模糊性以及持续改进。介绍了无限代理的概念，即能够将记忆信息转移到新领域或场景中。
2.2 Agent AI with Large Foundation Models：探讨了大型基础模型在Agent AI中的应用，包括幻觉、偏差与包容性、数据隐私与使用、可解释性与解释性、推理增强和法规。
2.3 Agent AI for Emergent Abilities：探讨了交互式代理AI系统中的泛化挑战，介绍了混合现实与知识推理交互的概念，以促进代理与人类在现实世界环境中的协作。
3 Agent AI Paradigm
3.1 LLMs and VLMs：讨论了如何利用LLMs和VLMs来引导Agent AI系统的组件，强调了LLMs在任务规划和VLMs在视觉编码方面的能力。
3.2 Agent Transformer Definition：提出了一种统一的代理变换器模型，该模型可以接受视觉、语言和特定于代理的标记作为输入。
3.3 Agent Transformer Creation：概述了创建代理变换器模型的过程，包括在领域内定义目标和通过监控与反馈进行持续改进。
4 Agent AI Learning
4.1 Strategy and Mechanism：讨论了训练Agent AI系统的不同策略和机制，包括强化学习、模仿学习、传统RGB方法、上下文学习和代理系统中的优化。
4.2 Agent Systems (zero-shot and few-shot level)：介绍了Agent AI模块和基础设施，强调了开发高效和有效的代理系统的需求。
4.3 Agentic Foundation Models (pretraining and finetune level)：探讨了预训练基础模型在Agent AI中的应用，强调了它们在多样化用例中的适用性。
5 Agent AI Categorization
5.1 Generalist Agent Areas：讨论了能够在多种情境和模态中与用户交互的通用代理。
5.2 Embodied Agents：探索了设计用于与物理世界交互的具身代理，包括行动代理和交互式代理。
5.3 Simulation and Environments Agents：讨论了在模拟环境中学习和交互的代理，这对于训练和测试AI系统至关重要。
5.4 Generative Agents：探讨了能够在游戏和混合现实设置中生成内容的生成性代理。
5.5 Knowledge and Logical Inference Agents：讨论了整合知识和逻辑推理以解决智能特定方面的代理。
5.6 LLMs and VLMs Agent：探讨了作为任务规划和推理代理的LLMs和VLMs在各个领域的应用。
6 Agent AI Application Tasks
6.1 Agents for Gaming：讨论了Agent AI在游戏中的应用，包括NPC行为、人与NPC的交互、基于代理的游戏分析和游戏场景合成。
6.2 Robotics：探索了Agent AI在机器人技术中的应用，重点在于视觉运动控制、基于语言的操控和技能优化。
6.3 Healthcare：讨论了Agent AI在医疗保健中的潜力，包括诊断代理、知识检索代理和远程医疗与监测。
6.4 Multimodal Agents：探讨了多模态代理在图像-语言理解和生成、视频和语言理解和生成以及多模态代理中的能力。
6.5 Video-language Experiments：介绍了将预训练的图像LLMs转换为视频理解的实验，强调了当前方法的潜力和局限性。
6.6 Agent for NLP：讨论了LLM代理、通用LLM代理、遵循指令的LLM代理以及NLP任务中的实验和结果。
7 Agent AI Across Modalities, Domains, and Realities
7.1 Agents for Cross-modal Understanding：讨论了跨模态理解的挑战，以及视觉编码器和LLMs联合调整的必要性。
7.2 Agents for Cross-domain Understanding：探讨了创建能够在不同领域操作的通用代理的挑战。
7.3 Interactive agent for cross-modality and cross-reality：讨论了开发能够在不同现实之间理解和执行任务的AI代理的发展。
7.4 Sim to Real Transfer：解决了在模拟中训练的模型在现实世界中表现不佳的“模拟到现实”问题，讨论了领域随机化、领域适应和改进模拟现实的技术。
8 Continuous and Self-improvement for Agent AI
8.1 Human-based Interaction Data：讨论了使用人类交互数据改进AI代理的方法，包括额外的训练数据、人类偏好学习和安全训练。
8.2 Foundation Model Generated Data：探讨了从基础模型生成训练数据的方法，包括指令调优方法和视觉-语言对。
9 Agent Dataset and Leaderboard
9.1 “CuisineWorld” Dataset for Multi-agent Gaming：介绍了用于多智能体游戏的CuisineWorld数据集，旨在评估多智能体系统的协作效率。
9.2 Audio-Video-Language Pre-training Dataset：介绍了VideoAnalytica基准测试，重点是对视频演示进行复杂推理任务的评估。
10 Broader Impact Statement
讨论了Agent AI在各个行业中的潜在积极影响，以及开发解决方案以应对现实世界问题的重要性。
11 Ethical Considerations
强调了Agent AI的伦理考虑，包括负责任的AI指南、透明度和最小化现实世界场景中的风险。
12 Diversity Statement
强调了多模态和具身AI中挑战和解决方案的多样性，旨在通过探索各种主题和观点来构建一个多样化的社区。

```note
挺大而全的  但是由于是2024年01的依然有点老
```

2. 2024.01 The Rise and Potential of Large Language Model Based Agents: A Survey
核心是启发式综述和一些讨论

由复旦NLP和米哈游公司共同撰写，从AI Agent的历史出发，全面梳理基于大型语言模型的智能代理现状
包括LLM-based Agent的背景、构成、应用场景以及备受关注的代理社会。
同时，探讨了Agent相关的前瞻开放问题，对相关领域的未来发展趋势具有重要价值。

长期以来，人类一直在追求等同于或超越人类水平的人工智能（AI），而Agent则被认为是大有可为的工具。人工智能代理是能够感知环境、做出决策和采取行动的人工实体。人们为开发智能代理做出了许多努力，但主要集中在算法或训练策略的改进上，以提高特定能力或特定任务的性能。实际上，社会各界缺乏的是一个通用而强大的模型，作为设计能适应各种场景的人工智能代理的起点。由于大型语言模型（LLM）所展示的多功能性，它们被视为人工通用智能（AGI）的潜在火花，为构建通用人工智能代理带来了希望。许多研究人员利用 LLM 作为构建人工智能代理的基础，并取得了重大进展。在本文中，我们将对基于 LLM 的代理进行全面研究。

我们首先追溯了代理概念从哲学起源到在人工智能中的发展，并解释了为什么 LLM 适合作为代理的基础。

在此基础上，我们提出了基于 LLM 的代理的一般框架，包括三个主要部分：大脑、感知和行动。

随后，我们从三个方面探讨了基于 LLM 的代理的广泛应用：单代理场景、多代理场景和人-代理合作。

随后，我们深入探讨了代理社会，探索了基于 LLM 的代理的行为和个性、代理社会中出现的社会现象以及它们对人类社会的启示。最后，我们讨论了这一领域的几个关键议题和悬而未决的问题。

1.1 早在 18 世纪，哲学家丹尼斯·狄德罗提出“如果一只鹦鹉能回答所有问题，它就可以被认为是聪明的”

代理涵盖了五个层次，描绘了从自然语言处理到通用人工智能的研究进展
**即语料库、互联网、感知、具身和社会**
纯语言模型建立在第二层次，使用互联网规模的文本输入和输出
如果赋予agent扩展的感知空间和行动空间，它们有可能达到第三和第四层次
还可以通过合作或竞争来处理更复杂的任务，可能实现第五个层次。

2.1 人工智能代理的起源
从哲学角度探讨代理的概念起源，其核心是具有行动能力的实体。在人工智能领域，代理被视为能够感知、决策和响应的人工实体。
2.2 智能体研究中的技术趋势
总结了人工智能代理发展的几个阶段，包括符号代理、反应型代理、基于强化学习的智能体、具有迁移学习和元学习的智能体，以及基于大型语言模型的智能体，分析了各阶段的特点和局限性。
2.3 为什么大型语言模型适合作为智能体大脑的主要组成部分
阐述了LLM具备自主性、反应性、主动性和社会能力等关键属性，使其非常适合担任人工智能代理的主要组件或控制器。
3 智能体的诞生：基于大型语言模型的智能体构建
收到agent定义启发，我们提出了基于大型语言模型的代理人的一般概念框架，
包括三个关键部分：**大脑（brain）、感知（perception）、行动（action）**，该框架可以根据不同的应用进行定制

3.1 大脑主要由LLM组成，负责信息处理、决策等；大脑是人工智能代理人的核心，因为它不仅存储关键的记忆、信息和知识，还承担信息处理、决策、推理和规划等基本任务。它是影响代理人是否能表现出智能行为的关键决定因素。

3.2 感知模块扩展代理的感知空间；感知空间从仅文本扩展到包括文本、声音、视觉、触觉、嗅觉等多种感官模式的多模态空间

3.3 行动模块扩展代理的行动空间。环境交互。具备文本输出、进行具身行动和使用工具，以便更好地响应环境变化并提供反馈，甚至改变和塑造环境。

4 实践中的智能体: 利用人工智能造福人类
探讨了基于LLM的代理在
4.1 单代理场景  讨论它们在基于文本的任务和模拟探索环境中的表现，强调它们在处理特定任务、推动创新以及展示类人生存技能和适应能力方面的能力
4.2 多代理场景  多代理系统应用中，代理之间的互动方式，这些代理进行合作、谈判或竞争。无论互动模式如何，代理共同努力朝着共享目标前进
4.3 人-代理合作中  隐私安全、伦理限制和数据不足等方面的潜在局限性

5 代理社会：从个体性到社会性
研究了基于LLM的代理的行为和个性、代理社会中出现的社会现象以及它们对人类社会的启示，设想了由AI代理组成的社会场景。
5.1 这些代理是否表现出类人行为、具有人格特征

6 Discussion
6.1 LLM研究与代理研究之间的互惠互利
讨论了LLM研究和代理研究如何相互促进，推动彼此领域的发展。
6.2 基于LLM代理的评估
从效用、社交性、价值观和持续演变能力四个维度探讨了对基于LLM的代理的评估方法。
6.3 安全性、可信度以及基于大型语言模型代理的其他潜在风险
分析了基于LLM的代理存在的一些风险，如对抗鲁棒性、可信性、误用、失业威胁等，并讨论了相应的缓解策略。
6.4 扩大代理数量
讨论了增加代理数量的潜在优势和挑战，以及预先确定和动态扩展的方法。
6.5 开放问题
探讨了基于LLM的代理是否代表通用人工智能的潜在路径、从虚拟模拟环境到物理环境的挑战、人工智能代理的集体智能、服务型代理等未解决的问题。
7 结论
论文全面系统地概述了基于LLM的智能体领域，从哲学起源到技术发展，从框架构建到应用探索，再到社会影响和未来展望，旨在为相关领域的研究人员和从业人员提供灵感和参考。
```note
比上述课程更宏观的理论 输入的模态拓展  输出的物理拓展  最终形成一个agent社会   至于框架本质无殊 还是一个核心处理模块+输入+输出
```

3. 2023.08 A survey on large language model based autonomous agents

由中国人民大学高瓴人工智能学院研究团队撰写。系统综述了基于大语言模型（LLM）的自主Agent研究进展。

传统自主代理依赖有限知识在封闭环境中训练，与人类学习模式存在显著差异，而LLM通过海量网络知识获取展现出类人智能潜力。

论文从构建、应用与评估三方面展开分析：在构建层面，提出统一框架整合记忆、规划与工具使用等核心模块，通过外部知识库和工具调用增强LLM的决策能力。

abstract
先前的研究关注于具有有限知识的智能体在隔绝环境下的训练，这与人类学习方式大相径庭，因此很难像人类一样做决策。最近，LLMs通过获取的丰富web知识，在完成人类等级只能上展现出非凡的潜力。本文对该领域研究进行了全面的调查，作者首先讨论了基于LLMs的自主智能体的构建，提出一个大一统的框架，接着作者阐述了基于LLMs的自主智能体在社科、自然科学和工程领域的各种应用，最后作者深入研究了基于LLMs的自主智能体常用的评估策略。基于这些研究，作者也提出了这个领域的一些挑战和未来方向。
1.1 Introduction
自主智能体一直是实现通用人工智能（AGI）的有前途的方法。然而，先前的研究通常基于隔绝环境和简单启发式策略，这限制了它们的决策能力。最近，大型语言模型（LLMs）通过获取丰富的网络知识，在展现类人类智能方面取得了突破。基于LLMs的自主智能体能够利用其类人类能力来完成任务，这推动了学术界和工业界对这一领域的研究兴趣。本文通过构建、应用和评估三个方面，对基于LLMs的自主智能体进行了全面的综述。
2.2 LLM-based Autonomous Agent Construction
基于LLM的自主智能体需要合理的架构设计来最大化其能力。文章提出一个**统一框架，包括画像模块、记忆模块、规划模块和动作模块。** 画像模块定义智能体的角色，记忆模块帮助智能体积累经验，规划模块赋予智能体分解任务的能力，动作模块将智能体的决策付诸实践。此外，智能体的能力获取策略分为基于微调和无需微调两类。
3.3 LLM-based Autonomous Agent Application
基于LLM的智能体在多个领域展现出巨大潜力。在社会科学领域，它们可以用于心理学模拟、政治学与经济学研究、社交模拟、法律决策支持和研究助手等。在自然科学领域，它们适用于文档与数据管理、实验助手和自然科学教育。在工程领域，它们在土木工程、计算机科学与软件工程、工业自动化以及机器人与人工智能等方面具有应用价值。
4.4 LLM-based Autonomous Agent Evaluation
评估基于LLM的自主智能体有两种常见策略：**主观评估和客观评估。** 主观评估包括人类标注和图灵测试，适用于没有评估数据集或难以量化的场景。客观评估采用定量指标，包括任务成功指标、人类相似度度量和效率指标，以及评估协议和基准。
5.5 Related Surveys
尽管大语言模型领域已有大量综述，但专门针对基于LLM的自主智能体的综述尚属首次。本文整理了100多篇相关著作，涵盖构建、应用和评估。
6.6 Challenges
基于LLM的自主智能体面临多个挑战，包括角色扮演能力、泛化人类对齐、提示工程的鲁棒性、幻觉问题、知识边界和效率问题。
```note
文章更老2023 感觉不够宏观 记忆啦规划啦动作啦 完全分属思考核心和输出模块  但是画像挺有意思  本质上是规定这个思考核心的目标 即课程里的goal
```

4. 2024.02 Large Language Model based Multi-Agents: A Survey of Progress and Challenges
重点是一张树形发展图 一些baseline和dataset

本文由南方科技大学等联合撰写。分析了LLM多智能体所模拟的领域和环境、这些智能体的特征及其通信方式，以及促进智能体能力发展的机制。

还总结了该领域常用的数据集和基准，以便有兴趣的研究人员能够方便地获取。为读者提供关于LLM多智能体系统研究进展和挑战的深刻见解。

摘要
本文深入探讨了基于大型语言模型（LLM）的多智能体系统，重点研究了其在复杂问题解决和世界模拟方面的进展。文章分析了这些系统所模拟的领域和环境，智能体的描述和通信方式，以及促进智能体能力增长的机制。此外，文章还总结了常用的数据集和基准。

1 Introduction
单智能体发展：基于LLM的智能体在理解和生成人类指令、促进复杂交互和决策方面迅速发展。
多智能体系统优势：通过集体智能和多个智能体的专门配置文件和技能，解决更复杂的任务。
LLM能力：交流能力和广泛知识，专注于特定任务的潜力。
应用领域：软件开发、多机器人系统、社会模拟、政策模拟、游戏模拟等。
2 Background
2.1 单智能体系统
决策思维：基于LLM的智能体在提示引导下分解复杂任务，进行多路径探索和学习。
工具使用：利用外部工具和资源增强功能。
记忆力：上下文学习和长期内存，保持上下文一致性。
2.2 单智能体与多智能体系统对比
单代理：关注内部机制和与外部环境的相互作用。
多代理：强调不同的主体概况、主体间的互动和集体决策过程。
3 剖析LLM-MA系统
3.1 代理与环境接口
沙盒环境：模拟或虚拟环境，代理可以自由交互。
物理环境：真实世界环境，代理与物理实体交互。
无环境：代理不与任何环境交互，专注于通信。
3.2 代理个人资料
预定义：系统设计人员显式定义。
模型生成：按模型创建，如大型语言模型。
数据派生：基于现有数据集构建。
3.3 代理通信
通信范式：协作、辩论、竞争。
通信结构：分层、分散、集中、消息池。
通信内容：通常采用文本形式。
3.4 代理能力获取
反馈：来自环境、代理交互、人类反馈。
自我调节：记忆、自我进化、动态生成。
4 应用
4.1 问题解决
软件开发：模拟不同角色，协作解决复杂挑战。
具身代理：多个机器人协作执行物理任务。
科学实验：多智能体协作进行科学实验，人类监督关键。
科学辩论：智能体辩论增强集体推理能力。
4.2 世界模拟
社会模拟：模拟社会行为，探索社会动态。
游戏模拟：测试博弈论假设，模拟人类互动。
心理学：直接应用心理学实验或模拟社会行为。
经济模拟：模拟经济和金融环境中的行为。
推荐系统：模拟用户和商品交互，传播偏好。
政策制定：模拟政策制定及其潜在影响。
疾病传播模拟：模拟疾病传播和人类反应。
5 实施工具和资源
5.1 多智能体框架
MetaGPT：将人类工作流程嵌入语言模型代理。
CAMEL：促进代理之间的自主合作。
AutoGen：通用框架，允许使用语言模型创建应用程序。
5.2 数据集和基准
问题解决场景：评估多智能体合作或辩论的规划和推理能力。
世界模拟场景：评估模拟世界与现实世界的一致性或分析代理行为。
结论
本文系统总结了基于LLM的多智能体系统的研究，包括其基本方面和面临的挑战。文章强调了这些系统在复杂问题解决和世界模拟方面的潜力，并提供了常用的数据集和基准以供进一步研究。

```note
启发式列举了可能的应用领域  研究要用的基准和data
```

5. 2024.05 Personal LLM Agents: Insights and Survey about the Capability, Efficiency and Security

本篇论文由清华大学人工智能产业研究院联合多家公司撰写。聚焦于个人大型语言模型（LLM）智能体，探讨了其架构、能力、效率和安全性等关键问题。

通过对领域专家的意见收集和分析，总结了个人LLM代理的关键组件和设计选择，并深入讨论了实现智能、高效和安全的个人LLM代理所面临的挑战，以及相应的解决方案。

正文
前言
研究背景：科幻小说中的智能个人助理逐渐成为现实，但现有IPA在灵活性和可扩展性方面仍有限。LLM的出现为IPA发展带来新机遇，可显著增强其能力和实用性。
目前存在的问题：现有IPA在任务执行、上下文感知和记忆方面存在不足，且难以支持大规模任务。
目前已有的解决方案：LLM能够使IPA自主解决复杂问题，提供更智能的服务。
研究目标：探讨个性化LLM代理的路线图、设计选择、主要挑战和可能的解决方案，重点关注“个性化”部分，包括个人数据的管理和利用、个人资源的使用、在个人设备上的部署以及提供个性化服务。
智能个人助理(IPA)的简史
IPA历史的时间轴视图：分为四个阶段，从早期的语音识别技术到现代智能手机上的虚拟助理服务，再到LLM时代的智能聊天机器人和基于LLM的IPA软件。
IPA的技术观点：以任务自动化能力为重点，介绍四种主要类型的技术：基于模板设计的技术、基于监督学习的技术、基于强化学习的技术和早期采用基础模型的技术。
个性化LLM代理：定义和见解
定义：将个性化LLM代理定义为与个人数据、个人设备和个人服务深度集成的基于LLM的智能体，旨在帮助用户减少重复和繁琐的工作，更多地专注于有趣和重要的事务。
分析方案：通过调查领域专家的见解，总结个性化LLM智能体的关键组成部分、智力水平分类法和专家对常见问题的意见。
个性化LLM代理——基础的能力
任务执行：讨论了任务执行的自动化方法、代理框架和评估方法。
上下文感知：探讨了上下文感知的感知源和感知目标，以及如何通过硬件传感器、软件传感器和多传感器组合实现上下文感知。
记忆力：分析了记忆力的获取、管理和利用，包括日志记录、记忆推理以及原始数据管理、记忆增强的LLM推理和智能体自进化。
个性化LLM代理——效率
推理：讨论了模型压缩技术、推理加速技术、内存压缩技术和能耗优化技术，以提高LLM推理的效率。
定制：探讨了提升上下文加载效率和LLM微调效率的方法，包括参数高效的微调技术、高效的优化器设计和训练数据管理。
记忆操作：关注高效的外部内存检索，包括提升搜索效率和优化工作流效率。
安全和隐私
保密性：讨论了本地数据处理、同态加密、数据屏蔽和信息流控制等方法，以保护用户隐私。
诚实性：分析了对抗攻击、后门攻击和提示注入攻击等新型攻击形式，以及相应的防御方法。
可靠性：从问题、改进和检测三个角度讨论了LLM的可靠性，包括幻觉、未识别的操作和连续可靠性问题，以及对齐、自我反省和检索增强等改进方法，还有验证、解释和中间特征分析等检测方法。
结论与展望
研究现状：目前关于个性化LLM代理的研究还处于早期阶段，任务执行能力相对不足，支持的功能范围较窄，且在效率、可靠性和可用性方面存在许多关键问题。
未来展望：需要共同努力为个性化LLM代理建立整个软件/硬件堆栈和生态系统，同时确保其良性和辅助性质。

```note
聚焦个人agent的问题 突出的是个性化
论文展望的是个性化智能体将成为 toC 终端的主要软件范式。
探讨核心在于架构、能力、效率和安全性
```

6. 2024.02 Understanding the planning of LLM agents: A survey

本篇论文由中国科学技术大学和华为诺亚方舟实验室的团队撰写。

主要梳理了LLM-based Agent 中的规划（planning）能力。

对基于大型语言模型（LLM）的智能体规划进行了系统性回顾，提出了一个关于 LLM-Agent 规划的现有工作的分类体系，
将其分为五个方向：**任务分解、计划选择、外部辅助模块、反思和记忆**
每个方向都进行了全面分析，并讨论了该研究领域的挑战。作者旨在提供一个关于基于 LLM 的代理规划能力的系统性视角，这在该领域尚属首次。论文还评估了在四个基准测试上的几种代表性方法，为未来的研究方向提供了见解。

1.1 引言 (Introduction)
自主智能代理：被定义为能够完成特定任务的智能实体。它们通过感知环境、规划和执行动作来实现目标。
规划的重要性：规划是代理最关键的能力之一，它要求代理进行复杂的理解、推理和决策过程。
规划任务的一般表述：在给定的时间步 ，环境表示为 ，动作空间为 ，任务目标为 ，以及在时间步 的动作为 ，规划过程可以被表达为生成一系列动作的序列： 。其中， 和 分别代表LLM的参数和任务的提示。
传统方法的局限性：以往的工作主要依赖于符号方法或基于强化学习的方法，如规划领域定义语言（PDDL）或策略学习。这些传统方法有其局限性，例如符号方法需要将自然语言描述的问题转换为符号建模，这可能需要人类专家的努力，而且缺乏容错性。强化学习方法通常需要与环境的大量样本（交互）来学习有效策略，这在数据收集耗时或成本高昂的场景中可能不切实际。
LLM的潜力：近年来，大型语言模型（LLM）的出现标志着一个范式的转变。LLM在多个领域取得了显著的成功，展示了在推理、工具使用、规划和指令跟随方面的重要智能。这种智能为将LLM作为代理的认知核心提供了可能性，从而有潜力提高规划能力。
本文工作：尽管已有调查尝试总结LLM的技术，但文献中往往缺乏对规划能力的详细分析。本调查旨在分析最新的研究工作，讨论优势和局限性，并提供对基于LLM的代理规划能力的系统性视角。


2.2 任务分解 (Task Decomposition)

现实世界中的任务通常是复杂和多步骤的，直接通过单步规划过程来解决复杂任务是一项巨大挑战。任务分解通过将复杂任务分解为多个简单子任务，使得规划过程更加可行。

分解方法分类：任务分解方法主要分为两类：
分解优先方法（Decomposition-First Methods）：首先将任务分解为子目标，然后依次为每个子目标制定计划。
交错分解方法（Interleaved Decomposition Methods）：在任务分解和子任务规划之间进行交错，每次只揭示当前状态的一个或两个子任务。

分解优先方法的代表工作：
HuggingGPT：LLM作为控制器，负责将人类输入的任务分解为子任务，选择模型，并生成最终响应。
Plan-and-Solve：将原始的“让我们逐步思考”转变为两步提示指令：“我们首先制定计划”和“我们执行计划”。
ProgPrompt：将自然语言描述的任务转化为编码问题，将每个动作形式化为函数，每个对象表示为变量。
交错分解方法的代表工作：
Chain-of-Thought (CoT)：通过构建的轨迹指导LLM对复杂问题进行推理，利用LLM的推理能力进行任务分解。
Zero-shot CoT：使用“让我们逐步思考”的指令，解锁LLM的零样本推理能力。
ReAct：将推理和规划解耦，交替进行推理（思考步骤）和规划（行动步骤）。
讨论：
分解优先方法的优势在于创建了子任务与原始任务之间的强关联，降低了任务遗忘和幻觉的风险。但需要额外的调整机制，以避免某个步骤的错误导致整体失败。
交错分解方法可以根据环境反馈动态调整分解，提高了容错性。但对于复杂任务，过长的轨迹可能导致LLM产生幻觉，偏离原始目标。
挑战：尽管任务分解显著提高了LLM代理解决复杂任务的能力，但仍存在挑战，包括任务分解引入的额外开销、时间成本和计算成本，以及LLM的上下文长度限制。
3.3 多计划选择 (Multi-Plan Selection)

由于任务的复杂性和LLM固有的不确定性，对于给定任务，LLM代理可能会生成多种不同的计划。多计划生成涉及利用生成模型解码过程中的不确定性，通过不同的采样策略来产生多个候选计划。 - Self-consistency：采用简单的直觉，即复杂问题的解决方案很少是唯一的。通过温度采样、top-k采样等策略，获得多个不同的推理路径。 - Tree-of-Thought (ToT)：提出“采样”和“提议”两种策略来生成计划。LLM在解码过程中会采样多个计划，并通过少量示例提示生成各种计划。 - Graph-of-Thought (GoT)：在ToT的基础上增加了思想的转换，支持任意思想的聚合。 - LLM-MCTS 和 RAP：利用LLM作为启发式策略函数，通过蒙特卡洛树搜索（MCTS）算法来获取多个潜在动作。

最优计划选择：在候选计划中选择最优计划时，采用了多种启发式搜索算法。
Self-consistency：使用简单的多数投票策略，将得票最多的计划视为最优选择。
Tree-of-Thought (ToT)：支持树搜索算法，如广度优先搜索（BFS）和深度优先搜索（DFS），使用LLM评估多个动作并选择最优动作。
LLM-MCTS 和 RAP：也使用树结构辅助多计划搜索，但它们采用MCTS算法进行搜索。
LLM A：利用经典的A算法协助LLM搜索，使用当前位置到目标位置的切比雪夫距离作为启发式成本函数来选择最优路径。
讨论：
多计划选择的可扩展性显著优势在于提供了在广阔搜索空间中更广泛探索潜在解决方案的能力。
然而，这种优势伴随着计算需求的增加，尤其是对于具有大量token计数或计算的模型，这在资源受限的情况下尤为重要。
LLM在计划评估中的作用引入了新的挑战，因为LLM在任务排名方面的表现仍在审查中，需要进一步验证和微调其在此特定情境下的能力。
LLM的随机性质为选择过程增加了随机性，可能影响所选计划的一致性和可靠性。

4.4 外部规划器辅助规划 (External Planner-Aided Planning)

尽管LLM在推理和任务分解方面展现出了强大的能力，但在面对具有复杂约束的环境时，例如数学问题求解或生成可执行动作，仍然面临挑战。

方法分类：根据引入的规划器类型，这些方法可以分为两类：
符号规划器（Symbolic Planner）：基于形式化模型，如PDDL，使用符号推理来找到从初始状态到目标状态的最优路径。
神经规划器（Neural Planner）：通过强化学习或模仿学习技术训练的深度模型，针对特定领域展现出有效的规划能力。
符号规划器的代表工作：
LLM+P：通过结合基于PDDL的符号规划器，使用LLM将问题组织成PDDL语言格式，并利用Fast Downward solver进行规划。
LLM-DP：特别为动态交互环境设计，将环境反馈信息形式化为PDDL语言，并使用BFS solver生成计划。
LLM+PDDL：在LLM生成的PDDL模型中增加手动验证步骤，并提出使用LLM生成的计划作为局部搜索规划器的初始启发式解。
LLM+ASP：将自然语言描述的任务转换为ASP问题，然后使用ASP求解器CLINGO生成计划。
神经规划器的代表工作：
CALM：结合了语言模型和基于RL的神经规划器，使用语言模型生成候选动作，然后通过DRRN策略网络重新排序以选择最优动作。
SwiftSage：将规划过程分为快速思考和慢速思考，快速思考通过模仿学习训练的DT模型实现，慢速思考则涉及LLM基于当前状态的推理和规划。
讨论：
在这些策略中，LLM主要扮演支持角色，其主要功能包括解析文本反馈并提供额外的推理信息以协助规划，特别是在解决复杂问题时。
传统的符号AI系统在构建符号模型时复杂且依赖于人类专家，而LLM可以加速这一过程，有助于更快更优地建立符号模型。
符号系统的优势包括理论完备性、稳定性和可解释性。将统计AI与LLM结合，有望成为未来人工智能发展的主要趋势。


5.5 反思和精炼 (Reflection and Refinement)
反思和精炼是规划过程中不可或缺的组成部分，它们增强了LLM代理规划的容错能力和错误纠正能力。由于LLM在规划过程中可能产生幻觉或在复杂问题上推理能力不足，导致错误或陷入“思维循环”，反思和总结失败有助于代理纠正错误并在后续尝试中打破循环。

Self-refine： 利用迭代过程，包括生成、反馈和精炼。在每次生成后，LLM为计划产生反馈，促进基于反馈的调整。
Reflexion： 扩展了ReAct方法，通过引入评估器来评估轨迹。LLM在检测到错误时生成自我反思，帮助纠正错误。
CRITIC： 使用外部工具，如知识库和搜索引擎，来验证LLM生成的动作。然后利用外部知识进行自我纠正，显著减少事实错误。
InteRecAgent： 使用称为ReChain的自我纠正机制。LLM用于评估由交互推荐代理生成的响应和工具使用计划，总结错误反馈，并决定是否重新规划。
LEMA： 首先收集错误的规划样本，然后使用更强大的GPT-4进行纠正。这些纠正后的样本随后用于微调LLM代理，从而在各种规模的LLaMA模型上实现显著的性能提升。
讨论： - 自我反思策略类似于强化学习的原则，其中代理作为决策者，环境反馈触发策略网络的更新。然而，与深度强化学习通过修改模型参数来更新不同，在LLM代理中，这种更新是通过LLM自身的自我反思来实现的，最终形成文本形式的反馈。 - 这些文本反馈可以作为长期和短期记忆，通过提示影响代理后续的规划输出。然而，目前还没有确凿的证据证明这种文本形式的更新最终能够使LLM代理达到特定目标。

6.6 记忆增强规划 (Memory-Augmented Planning)
记忆是提升代理规划能力的关键途径，可以帮助代理从经验中学习并适应新的情境。 - RAG-based Memory（基于RAG的记忆）： - 概念：使用检索增强生成（Retrieval-Augmented Generation, RAG）技术，将记忆以文本形式存储，并在需要时检索出来辅助规划。 - 方法：如MemoryBank、TiM 和 RecMind，这些方法通过文本编码模型将记忆编码为向量，并建立索引结构，以便在规划时检索与当前任务相关的经验。 - Embodied Memory（体现记忆）： - 概念：通过微调（fine-tuning）LLM，将代理的历史经验样本嵌入到模型参数中，从而增强记忆能力。 - 方法：如CALM 和 TDT，这些方法使用从代理与环境交互中收集的数据来微调模型，使其能够记住与规划相关的信息，并在规划任务中表现更好。 - 记忆更新方式： - RAG-based：提供了实时、低成本的外部记忆更新，但依赖于检索算法的准确性。 - Finetuning：提供了更大的记忆容量，但记忆更新成本较高，并且在保留细节方面存在挑战。 - 讨论： - 记忆增强的LLM代理在规划中表现出更强的增长潜力和容错能力，但记忆生成在很大程度上依赖于LLM自身的生成能力。 - 通过自我生成的记忆来提升较弱LLM代理的能力仍然是一个具有挑战性的领域。 - 挑战：尽管记忆增强LLM代理在规划方面表现出优势，但它们在记忆生成上仍然面临挑战，特别是在自我生成记忆方面。

7.7 评估 (Evaluation)
评估代理的规划能力是研究领域中的一个关键问题。作者调查了几种主流的基准测试方法，并将它们分为以下几类：

交互式游戏环境（Interactive Gaming Environments）：
提供基于代理动作的实时多模态反馈，如文本和视觉反馈。
例如Minecraft，代理需要收集材料制作工具以获得更多奖励，常用评价指标是代理创建的工具数量。
基于文本的交互环境（Text-based interactive environments）：
代理位于用自然语言描述的环境中，动作和位置有限。
常用评价指标是成功率或获得的奖励，例如ALFWorld和ScienceWorld。
交互式检索环境（Interactive Retrieval Environments）：
模拟人类在现实生活信息检索和推理的过程。
代理可以与搜索引擎和其他网络服务交互，通过搜索关键词或执行点击、前进、后退操作来获取更多信息，完成问答任务或信息检索任务。
交互式编程环境（Interactive Programming Environments）：
模拟程序员与计算机之间的交互，测试代理解决计算机相关问题的规划能力。
代理需要与计算机交互，通过编写代码或指令来解决问题。

实验：
作者在四个基准测试上进行了实验，以验证代表性方法的性能。这些基准测试包括ALFWorld、ScienceWorld、HotPotQA和FEVER，涵盖了交互式游戏和问答基准测试。
实验结果显示，性能随着成本的增加而提高，表明更详细的思考（即消耗更多的token）可以带来性能上的提升。
另外，对于复杂任务，示例（例如Zero-shot CoT和Few-shot CoT）对于LLM进一步理解任务至关重要。
反思（Reﬂexion）在提高成功率方面发挥了关键作用，尤其是在复杂任务中，显示了LLM具备错误纠正能力。
讨论：
现有的基准测试大多依赖于任务的最终完成状态，缺乏细粒度的逐步评估。
环境反馈通常是规则化的、简化的，并且与现实世界场景有距离。
未来的发展方向可能包括利用高智能模型如LLM来设计更现实的评估环境。

8.8 结论和未来方向 (Conclusions and Future Directions)
进展总结：自LLM展现出智能以来，使用LLM增强代理规划能力的研究受到了越来越多的关注。作者概述了主要的研究方向，并在前文中对各种方法进行了详细比较和分析。
实验结果：作者在四个基准测试上进行了实验，比较了几种代表性方法的有效性，并指出随着投入成本的增加，性能也随之提高。
挑战：尽管这些研究在规划能力上取得了增强，但仍存在一些重大挑战：
幻觉问题（Hallucinations）：LLM在规划过程中可能会产生幻觉，导致非理性的计划或无法遵循复杂指令。
生成计划的可行性：与基于符号的人工智能相比，LLM在优化过程中可能难以遵守复杂约束，导致生成的计划缺乏可行性。
计划的效率：现有LLM代理的规划过程可能没有考虑生成计划的效率，未来的发展可能需要引入额外的效率评估模块。
未来方向：
多模态环境反馈：考虑集成多模态大型模型的发展，并重新审视相关的规划策略，以处理包括图像、音频等在内的多模态反馈。
细粒度评估：利用高智能模型如LLM设计更现实的评估环境，提供更细致的逐步评估，以更好地模拟现实世界场景。


```note
聚焦于agent的plan能力

plan分解方法有分解优先方法（创建了子任务与原始任务之间的强关联，但需要额外调整机制避免错误传播）
               交错分解方法（可根据环境反馈动态调整，提高容错性，但轨迹过长导致幻觉）
多plan选择 多计划生成、最优计划选择（多种启发式搜索算法选择最优计划

外部辅助plan 符号规划器（基于形式化模型） 神经规划器（用强化学习或模仿学习）

反思和精炼 类比RL 但是更新通过LLM自我反思实现，文本反馈可作为记忆影响后续规划，效果需进一步验证。

memory增强的plan  有RAG（实时、低成本更新，依赖检索算法）和 微调LLM即embedding（容量大但是微调成本高）

评估的话 整体是投入越多效果越好
交互式游戏环境：如Minecraft，评价指标为代理创建的工具数量。
基于文本的交互环境：如ALFWorld、ScienceWorld，评价指标为成功率或获得的奖励。
交互式检索环境：模拟信息检索和推理过程，评价指标为任务完成情况。
交互式编程环境：测试代理解决计算机问题的规划能力。

挑战：幻觉问题、生成计划的可行性、计划的效率。
未来方向：多模态环境反馈、细粒度评估。
```



### 单agent论文（3篇）：
1. **HuggingGPT**
   - 论文链接：https://arxiv.org/pdf/2303.17580
   - Github：https://github.com/microsoft/JARVIS

2023.03 HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face
微软亚洲研究院和浙江大学

核心：以非常工程的prompt控制 通过单一chatGPT调用丰富的hugging face模型，处理多模态复杂任务。
有意思的是 项目仓库名称就是钢铁侠的“贾维斯”

论文认为解决大型语言模型（LLMs）局限性，是迈向人工通用智能（AGI）的关键一步。
当前LLMs在处理复杂信息（如视觉和语音）、多子任务协作以及特定任务表现上存在不足。
HuggingGPT通过将AI模型功能以语言形式表示，使ChatGPT能够像大脑一样管理和调用这些模型。
其工作流程分为四个阶段：**任务规划、模型选择、任务执行和响应生成。**
目前，HuggingGPT已在HuggingFace上集成了数百个模型，涵盖24个任务类型。
实验表明，HuggingGPT能够处理多模态信息和复杂任务。
该项目已开源，但需要OpenAI API支持。

具体来说
步骤1：任务规划（Task Planning）
Prompt设计：综合使用“基于规范的指令”和“基于示范的解析”的方法。
基于规范的指令：对Task做了规范约束，必须是包含任务内容（task）、任务id（id）、任务依赖（dep）和参数（args）等四个要素的格式：[{"task": task, "id", task_id, "dep": dependency_task_ids, "args": {"text": text, "image": URL, "audio": URL, "video": URL}}]。
基于示范的解析：Prompt中提供若干“示例”（Demonstrations），激发大模型的基于上下文学习或者Few-shot learning能力，从而产生更好的理解和生成。
提供的信息：Prompt还会给LLM提供Available Task List和Chat Logs，即可用的HuggingFace模型列表和对话历史，提供更多、更准确的上下文信息。
步骤2：模型选择（Model Selection）
Prompt设计：比较直白，但同样给LLM提供了明确的“备选模型”（Candidate Models）的信息。
备选模型数据获取：
模型描述获取：从HuggingFace网站获取每个模型的功能、架构、领域、许可等各种信息，作为喂给LLM做模型筛选的输入。
模型初筛：由于Prompt有长度限制，根据步骤1的结果中的task字段，和模型描述中的功能或分类做匹配，得到该任务类型下的所有模型描述。
模型细筛：按照任务初筛之后，模型还是会很多，于是还要进行进一步的细筛。文章中提到是根据模型的下载量取TopN来实现，下载量很大程度上反应了该模型的质量和受欢迎程度，可以作为进一步筛选的标准。
步骤3：任务执行（Task Execution）
任务执行就是唤起选中的模型，执行算法，最后返回结果给到ChatGPT。这个过程不是由LLM执行的，从而不需要Prompt设计。
步骤4：响应生成（Response Generation）
Prompt设计：汇总各个模型的运行结果，给用户生成一个答案。Prompt需要描述整个过程和结果，先直接回答用户请求，然后描述任务过程和展示模型推理结果。

```note
核心是工程化的prompt写法 作为参考  论文也较取巧和古早
```

2. **KG-Agent**
   - 论文链接：https://arxiv.org/pdf/2402.11163

2024.02 KG-Agent: An Efficient Autonomous Agent Framework for Complex Reasoning over Knowledge Graph

核心是一个框架 用于提升LLM对知识图谱（KG）的推理能力
使其更好完成知识密集型任务

背景
LLM 在多跳推理和知识密集型任务上能力有限，而 KG 中存储着大量结构化知识三元组，但 LLM 难以有效利用其信息，现有基于 LLM 的 KG 推理方法如检索增强和协同增强存在诸多局限性。
技术方案
设计目标 ：打造自主推理方法，让智能体在推理时可主动决策无需人工协助；使较小模型（如 7B LLM）能有效执行复杂推理，不依赖闭源 LLM APIs。
技术贡献 ：
策划多功能工具箱，扩展 LLM 处理结构化数据的能力，设计了提取工具、逻辑工具、语义工具三种类型的工具，分别用于从 KG 中提取信息、对信息进行基本操作及拓展对 KG 的操作。
利用 KG 推理数据集合成基于代码的指令数据来微调 LLM，先是根据 KG 推理链生成程序，再合成指令数据。
提出基于工具选择和存储器更新的自主迭代机制，集成调谐的 LLM、多功能工具箱、基于 KG 的执行器和知识存储器，实现自主推理。
KG - Agent 架构
KG 工具箱 ：涵盖上述三类工具，共 12 个工具，可满足 KG 推理中的多种操作需求。
KG - Agent 指令微调 ：用现有 KGQA 数据集生成 KG 推理流程，进而合成指令数据集，用于微调轻量 LLM（LLaMA2 - 7B）。
KG 自主推理 ：包含 LLM、多功能工具箱、基于 KG 的执行器、知识记忆存储器四个组件。知识记忆存储器记录上下文和有用信息，工具选择规划器基于知识记忆选工具，记忆更新执行器执行工具后更新知识记忆，然后迭代进行工具选择和记忆更新，直至得到答案实体停止。
实验结果
域内任务 ：仅用 10K 样本调整较小的 LLM，在 WebQSP、CWQ、GrailQA 等数据集上，相比基于 LLM 的基线（使用更多原始训练集）获 F1 相对改善，如约用 36% 和 23% 原始训练集量，在 CWQ 和 GrailQA 上分别获 7.5% 和 2.7% F1 相对改善；与传统基于子图的推理方法比，LM 生成的 SPARQL 查询获更完整答案集且结构化查询能支持复杂操作。
域外任务 ：zero-shot 性能优于全数据监督微调模型，如在 WQ - Freebase 和 TQ - Wiki 上分别相对提高 9.7% 和 8.5% 精度；相比小型预训练语言模型，可在 zero - shot 设置中利用外部 KG 取得一致改进。
总结与展望
KG - Agent 是首个协同 LLM 和 KG 执行 KG 复杂推理的自主智能体框架，仅用较少调优样本和相对小的 LLM，性能优于多个基线。未来计划扩展框架以处理更多结构化数据类型，如数据库和表格等。

```note
一个用来处理KG推理的agent框架  个人要补一下KG的知识
```


3. **ReadAgent**
   - 论文链接：https://arxiv.org/pdf/2402.09727
   - demo：https://read-agent.github.io/

2024.02 A Human-Inspired Reading Agent with Gist Memory of Very Long Contexts
核心是长上下文或记忆的检索方式

针对大语言模型在处理长文档时面临的上下文长度限制问题，
提出了一种创新的人类启发式阅读代理（ReadAgent）框架。
该研究旨在借鉴人类交互式阅读长文档的自然方式，
通过**智能分段、要点记忆和交互式查找**，
显著扩展有效上下文窗口长度，提升长文档阅读理解性能。

一、研究背景与目标
大语言模型在处理长文档时，受限于上下文长度，输出变长往往导致性能下降。针对这一问题，ReadAgent通过模仿人类阅读行为，将长文本内容分块处理，并压缩关键信息以形成要点记忆，同时结合交互式查找策略，实现了对长文档的有效理解与回答。
二、ReadAgent核心流程
情节分页：将长文本内容智能分块，通过提示LLM（大语言模型）在阅读连续文本时自动识别断点，将两个断点之间的内容作为一个页面进行处理。这种分页方式既保留了文本的连贯性，又降低了单次处理的复杂度。
记忆要点：对每一页进行信息压缩，提示LLM对每一页进行总结，将确切内容缩短为要点，并将要点与上下文相关联。这种要点记忆方式既保留了文本的关键信息，又大大减少了存储和处理的开销。
交互式查找：结合要点和原文进行作答。LLM在上下文查找信息时，除了使用要点记忆进行快速定位外，还可以在原始文本中查找相关细节。ReadAgent提供了两种查找策略：一次并行查找所有页面（ReadAgent-P）和一次顺序查找一页（ReadAgent-S）。前者适合于快速获取多页面信息，后者则更适合于逐页精细阅读。
三、实验结果与分析
实验采用了QuALITY、NarrativeQA和QMSum三个长文档阅读理解测试集，基座模型选择了PaLM 2-L（上下文8K）和GPT-3.5。与基线方案RAG（检索增强生成）相比，ReadAgent在性能上实现了显著提升。评测指标包括LR-1（完全匹配百分比）和LR-2（完全匹配和部分匹配百分比），同时引入LLM作为裁判进行一致性判断。结果显示，ReadAgent在三个任务上均优于基线方案，且将有效上下文窗口扩展了3-20倍。
四、结论与启示
ReadAgent通过长文本分段存储和摘要技术，结合交互检索策略，在保持良好性能的同时显著减少了计算开销。该方案在两个方面为知识治理提供了自动化思路：情节分页和记忆要点可用于RAG方案的知识库优化；逐页答案生成则可直接用于减少计算资源消耗。这些启示对于提升大语言模型在长文档处理领域的应用性能具有重要价值。
综上所述，《A Human-Inspired Reading Agent with Gist Memory of Very Long Contexts》论文提出了一种创新的长文档阅读理解框架ReadAgent，通过智能分段、要点记忆和交互式查找技术，实现了对长文档的有效处理与理解。该研究不仅拓展了大语言模型在长文档处理领域的应用边界，也为知识治理和智能问答系统提供了新的解决方案。
```note
agent中memory方面的长效理解处理技术工作
```






### 多agent论文（5篇）：
1. **MetaGPT**
   - 论文链接：https://arxiv.org/pdf/2308.00352.pdf
   - GIthub：https://github.com/geekan/MetaGPT

2023.08 MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework

核心是多agent协作编程的framework

借用了meta-learning的learning-to-learn来生成框架，旨在生成结构化的输出，包括高质量的需求文档、设计图、流程图和具体的实现接口等。

MetaGPT为了能够实现上面的目的，需要建立在多个标准下构建的Agent，使得Agent能够按照相应的标准进行自主地完成问题分析、系统设计、代码生成、更新、执行和调试等工作。

Agent in SOPs
对于一个比较复杂的工程任务（例如让大模型写一个flappy bird游戏），需要通过扮演不同角色的大模型之间相互协同来完成这个庞大的任务。每个角色的大模型完成具体的一个子环节的工作
分别是
plan、
requirement analysis、
architectural design、
sys design、
coding、
test、
acceptance check（验收）

定义好各个Agent的角色后，需要设置工作流程（Work Flow）：

产品经理角色Agent：根据用户的产品需求进行深入分析，并将任务进行分解，形成结构化的需求文档（Product Requirements Document，PRD）；（preliminary functional breakdown）；
架构师角色Agent：接收到产品经理角色大模型生成的PRD，将其转换为具体的系统设计、数据结构设计、文件列表等；
项目管理角色Agent：将架构师设计的系统等待开发工作进行任务分解（具体的代码接口功能）；
软件工程师角色Agent：接收到具体的接口和功能，进行代码生成，包括类、方法等；
QA工程师角色Agent：对于实现好的功能函数和类进行测试。
最后交付给用户，由用户进行评价反馈。

核心协作方式：
一种类似多轮对话的形式进行的，但不同于传统的文本对话，这里多个Agent之间对话采用结构化的文档（documents）或图表（diagrams）作为对话内容。
共享消息池：按照流程，上一个Agent通常只会跟下一个Agent进行交流，然而下一个Agent可能无法获取到上一个Agent之前的Agent的信息。因此设置一个共享消息池，每一个Agent可以将其产生的结构化消息存入到该池中，也可以通过信息检索策略从中提取消息。
Self-reflection：代码生成可能会存在幻觉，因此借助执行器和Debug工具来不断地完善代码。

实验：HumanEval：包含164个人工编写的程序任务；
MBPP：包含427个Python程序任务；
SoftwareDev：本文提出的benchmark，包含70个经典的软件开发任务

不足：MetaGPT中的Agent角色是固定的，且Action空间和顺序也是固定的，即每个流程要执行的Action都是事先定义好的，工具的使用也是在固定的Action之后完成。因此大模型在思考和生成的整个流程是被固定的。
```note
感觉是古早时期 人类流程设计大于agent规划的一个框架尝试
```

2. **AutoAgents**
   - 论文链接：https://arxiv.org/pdf/2309.17288.pdf
   - Github：https://github.com/Link-AGI/AutoAgents

2023.09 AutoAgents: A Framework for Automatic AgentGeneration
核心是自适应地生成和协调多个专业agent，根据不同的任务构建不同的 AI 团队

当前LLM的深度知识和推理的任务仍面临挑战，
如避免幻觉、应用慢思考策略、确保可信度以及结合多领域知识和长期规划。
大多数多agent系统依赖手工或用户指定的智能体，限制协作应用的范围，且手动创建大量专家消耗资源。

框架概述：
AutoAgents包含草稿阶段和执行阶段。

草稿阶段通过三个预定义智能体（规划者、智能体观察者、计划观察者）生成定制的智能体团队和执行计划；
- 智能体生成：规划者根据任务内容生成智能体团队，并与智能体观察者沟通优化。每个智能体包含提示、描述、工具集和建议四个要素。
- 智能体观察者评估智能体的合理性、与任务的匹配度以及团队的完整性。
- 计划生成：规划者制定执行计划，并与计划观察者沟通优化。计划观察者验证计划的合理性、完整性以及与任务和智能体团队的匹配度。

执行阶段通过智能体间的协作和反馈优化计划并输出最终结果。
任务执行动作：执行计划包含单智能体的自我优化和多智能体的协作优化两种动作。
自我优化提升单智能体在特定任务中的能力，协作优化促进知识共享，完成跨学科任务。
通信与知识共享机制：采用垂直通信范式，由行动观察者协调任务分配和执行。引入短期记忆、长期记忆和动态记忆，分别用于记录单个动作的中间信息、历史动作轨迹和特定任务所需的关键信息。

实验设置：使用GPT-4 API进行实验，设置温度为0以确保可重复性。
草稿阶段的最大讨论次数为3次，执行阶段的单智能体自我优化和多智能体协作优化的最大迭代次数为5次。
开放性问答任务：使用MT-bench基准测试，包含80个高质量开放问题 采用FairEval和HumanEval评估回答质量，FairEval通过多种方法减少偏差，HumanEval由志愿者根据回答的帮助性、可靠性、准确性和详细程度进行评分
创意写作任务等等

讨论：（消融）
协作讨论：与仅由规划者生成的智能体相比，协作讨论更符合实际场景需求，在没有观察者时AutoAgents性能降低3%。
自我优化：使LLM能够自我评估和迭代改进答案，在没有时AutoAgents性能降低3%
协作优化：不同领域知识的整合，完成跨学科任务，缺少协作优化时，AutoAgents性能降低2%。
动态记忆：为特定任务提供关键信息，没有动态记忆时，AutoAgents性能降低1%。
```note
本篇时间古早
个人思考 通用的agent集群应该按照现实社会进行组织完成任务
即 每个专业都应该准备一个或一类集群agent 负责同类事情
多专业再按任务进行自适应灵活组织
同组织内共享历史经验 共同明确目标
组织架构上 agent应该视任务难度形成不同的多级树形管理 尽管牺牲效率和管理难度
同组织与顶层得的用户进行连续交互 控制效果
```

3. **Large Language Model based Multi-Agents: A Survey of Progress and Challenges**
   - 论文链接：https://arxiv.org/pdf/2402.01680.pdf

2024.02 多agent的进展与挑战 见agent综述第4篇


4. **AutoGen**
   - 论文链接：https://arxiv.org/pdf/2308.08155
   - Github：https://github.com/microsoft/autogen

2023.08 AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation
微软著名的少见的把多Agent调教到稳定的开源框架，且一直更新

abstract
AutoGen是一个开源框架，通过多个能够相互对话以完成任务的agent构建 LLM 应用程序。AutoGen 代理可定制、可对话，并且可以在多种模式下运行，这些模式结合了 LLM、人工输入、工具。使用 AutoGen 开发人员还可以灵活地定义agent交互行为。自然语言和计算机代码均可用于为不同的应用程序编写灵活的对话模式。AutoGen 是一个通用框架，可用于构建各种复杂程度和 LLM 功能的应用程序。实验证明了该框架在许多示例应用中的有效性，涉及数学、编程、问答、运筹学、在线决策、娱乐等领域。

introduction 
多智能体方法能够促进发散思维、提升事实性和推理能力，并提供验证。作者提出基于多智能体方法开发涵盖广泛领域和复杂度的LLM应用程序。
解决方案：AutoGen框架，基于多智能体对话实现，利用LLM的最新进展，通过对话编程范式简化复杂任务的开发。
关键问题：设计高效、可重用、可定制的单个智能体；开发简单、统一的界面以适应各种智能体对话模式。

framework

关键概念：
可对话智能体（Conversable Agents）：
- 具有特定角色，能够传递消息并与其他智能体收发信息。
- 支持多种能力，包括LLM、人工输入和工具执行。
- 支持自定义和扩展，能够通过对话进行多轮交互。

对话编程（Conversation Programming）

- 将复杂任务简化为多智能体对话，通过对话驱动的计算和控制流实现任务处理。
- 提供统一的对话接口和自动回复机制，支持自然语言和编程语言的融合控制。
- 支持静态和动态对话模式，通过自定义回复函数和函数调用实现灵活的对话流程。

application
A1: Math Problem Solving：通过AutoGen构建自主数学问题求解系统，支持多智能体协作和人工反馈，展示了强大的性能和灵活性。
A2: Retrieval-Augmented Code Generation and Question Answering：构建检索增强聊天系统，支持问答和代码生成，引入交互式检索机制。
A3: Decision Making in Text World Environments：在ALFWorld基准上开发双智能体系统，通过引入基础智能体增强决策能力。
A4: Multi-Agent Coding：构建多智能体编码系统，优化代码编写和安全性检查，显著提高工作效率。
A5: Dynamic Group Chat：支持动态群聊通信模式，通过角色扮演提示优化发言者选择和对话流程。
A6: Conversational Chess：开发对话式国际象棋游戏，支持多种游戏模式和规则验证。

Discussion
优势：AutoGen通过多智能体协作提升了性能，减少了开发代码量，简化了开发流程，支持模块化和人工参与。
未来方向：研究最优的多智能体工作流、开发高性能智能体、评估扩展性与安全性，以及探索自动化与人工控制之间的平衡。

使用建议：
优先使用内置智能体，考虑人工输入模式、终止条件等配置。
从简单的对话拓扑开始，尝试复用内置回复方法。
开发新应用时始终开启人工参与，逐步调整为自主模式。
结合其他库/包以优化性能。
```note
本篇时间古早
核心关注文中多agent的设计范式
1是统一的对话接口 收发函数  和自定义函数 和自动回复机制来实现对话驱动：agent之间一旦收到消息自动生成回复给发送消息的agent
2是控制流管理模式中支持编程和自然语言 且可混用
更有启发性的是文章展示的六个任务下的不同架构与通信图片
个人思考
正如通用LLM解决通用问题  现在考虑通用架构而非特定任务架构 来解决通用任务
```

5. **Agent Hospital**
   - 论文链接：https://arxiv.org/pdf/2405.02957

2024.05 Agent Hospital: A Simulacrum of Hospital with Evolvable Medical Agents
类比斯坦福小镇 清华提出模拟医院   在虚拟环境中模拟医患互动，来训练医生智能体。

两个重要的模块，即「病历库」和「经验库」

整个自我进化流程如下：
1）积累实例，总结经验；
2）直接向示例库添加正确的响应；
3）总结错误的经验，并重新测试；
4）将成功经验进一步抽象后，纳入经验库；
5）在推理过程中利用两个库检索最相似的内容进行推理。

实验中在某些领域的疾病识别准确率93%，高于人类专家

仿真环境 ：使用 Tiled 和 Phaser 框架设计的沙盒仿真环境，包含 16 个不同功能的区域（如分诊站、诊室、检查室）。

智能体角色
医疗专业人员智能体 ：包括医生和护士，分别承担不同的职责。
居民智能体 ：可能随机生病的潜在患者，生病后会主动就医。
规划与互动 ：
居民规划 ：包括日常规划和动态规划，涵盖从疾病发作到康复的整个患者旅程。
医疗专业人员规划 ：包括实践（治疗患者）和学习（研究病历和教科书）。


医疗任务 ：定义了三项主要任务以评估医疗智能体的性能：
检查决策 ：根据症状选择合适的医学检查。
诊断 ：根据症状和检查结果确定疾病。
治疗计划 ：根据诊断结果推荐治疗方案。

模拟医疗数据集 ：利用 LLM 生成，涵盖八种呼吸道疾病的详细病历。
医疗文档数据集 ：包含医学新闻和教科书，为智能体学习提供额外知识。

MedAgent-Zero 策略
医疗记录库 ：以问答对形式存储成功的医疗记录，辅助未来的决策。
经验库 ：积累从错误诊断中得出的原则，指导未来的治疗。
推理过程 ：通过检索记录和验证的经验增强智能体的提示，以改进决策。
记录与经验检索 ：利用密集检索技术获取最相关的记录和经验。
提示增强 ：将检索到的记录和经验组合成少量样本示例，用于智能体的推理。
```note
本篇时间较近
个人主要关注设定库经验后的自主学习过程
```


### 多agent社会模拟论文（7篇）：


1. **斯坦福小镇重点**
   - 论文链接：https://arxiv.org/abs/2304.03442 
   - 讲解视频：（1）https://b23.tv/CmDaFvh （2）https://b23.tv/hWCaQGh
   - GitHub: https://github.com/Turing-Project/SimAIWorld?tab=readme-ov-file

2023.04 Generative Agents :interactive simulacra of human behavior
一句话总结：用 LLM 驱动 Agent，将日常生活行为、记忆与反思机制融合，生成小镇中可信的人类社交行为。

What：LLM‑增强 agent 在交互沙盒中模拟人类行为。

Why：传统静态模拟难以展现自然流动的社交与记忆驱动行为。

技术动机：利用 LLM 强大的语言与记忆整理能力生成 believable 行为。

How：用自然语言记录 experiences → 定期反思合成高阶 memory → 动态检索驱动 planning 和 act。

优势：行为真实、有 emergent group interaction，用户可用自然语言互动。

缺点与潜力：规模仅数十 agent，成本高；未来可扩到大规模模拟／集群训练。

具体：
每个智能体会有一段描述：人物背景、社会关系、性格等即profile
在沙盒引擎的每个时间步，代理输出描述动作的自然语言语句，例如“xxx正在写日记”，
然后，这个语句被转化为影响沙盒世界的具体动作。
用户可以:化身进入游戏,或控制现有agent的思想
场景设置(互动)：action改变env里object的state

突发社会行为，消息传播diffusion 智能体间的对话传播
关系记忆 Relationship Memory  智能体会随着时间的推移形成新的关系，并记住它们与其他智能体的互动。

核心架构：一个agent
perceive+【mem+增强的mem+plan+reflect】+act 
生成式智能体架构 为开放世界中的行为提供一个框架

具体而言 
内存和检索 Memory and Retrieval
Memory Stream的构成：带时间帧的事件描述列表
retrieval = recency + importance + relevance
通过分配更高的分数来区分平凡记忆和核心记忆 并且细节是 直接要求语言模型输出一个整数分数是有效的

反思 Reflection 是必须的
树形结构 叶子是观察 顶层是反思 越接近根节点越抽象，越高级。

规划和反应 Planning and Reacting
保证更长时间的行为合理性
计划描述了智能体未来的行动序列，并帮助智能体的行为在一段时间内保持一致。计划包括地点、开始时间和持续时间。
与反思一样，计划也存储在记忆流中，并包含在检索过程中。这使得智能体在决定如何行动时，可以同时考虑观察、反思和计划
在每个时间步感知周围的世界，观察结果存到记忆流。用观察结果提示语言模型决定保持计划还是做出反应。

对话生成上 根据智能体对彼此的记忆来调节话语  A对B的记忆+A的目的=A对B说的话

将沙盒环境-区域和对象- 表示为树形数据结构，树中的边表示沙盒世界中的包含关系。
个人类比组合模式

评估：
通过"采访问题"来评估一个agent的保持自我认知、恢复记忆、制定计划、反应和反思
有三种消融的架构: 没有观察、没有反思、没有规划的架构
其中 no reflection 效果下降 
加上no plan 效果更下降 
加上no observe效果递减 
且最后没有observe时候效果下降最快
实验结论：“一个都不能少”

端到端评估
让25个agents过两天，会涌现什么行为？
agents会传播消息，形成关系，相互协作
但是记忆增加带来挑战：检索和行动  对环境的认识不准确导致错误
```note
启发意义在于 环境中的object 桌子椅子等 状态可变  那么是否可以将环境视为大实体 做成插件
计划方面的问题 计划应该以目的为导向 很多计划不一定在做之前确定固定时间或持续多久 文中已实现随环境动态变化
评估中的启发是 如果对agent使用component便于进行消融实验评估agent能力
畅想未来 202507黄仁勋在中国就说 具身agent就是未来方向 目前甚至有具身版本的小镇
```

2. **Agentsociety重点**
   - 论文链接：https://arxiv.org/abs/2502.08691 
   - 学习手册：https://agentsociety.readthedocs.io/en/latest/index.html
   - Github: https://github.com/tsinghua-fib-lab/AgentSociety

2025.02 AgentSociety: Large-Scale Simulation of LLM-Driven Generative Agents Advances Understanding of Human Behaviors and Society
清华大学团队 一句话总结：面向社会科学实验的 LLM-agent 大规模模拟平台，支持上万 agents 与社会现象研究。
大型社会模拟器 AgentSociety 1.0 版本，可精确模拟社会舆论传播、认知观点极化、公众政策响应等


What：10k+ LLM agent 与虚拟社会环境结合模拟互动。

Why：替代成本高、人为实验，实现可复制、规模化的社会研究。

技术动机：结合社会学理论、城市/经济环境建模与语言代理 agent。

How：AgentSociety 框架包括 agent 心智设计、现实环境模拟、强 simulation engine 支持百万级交互。

优势：真实还原 polarization、UBI、外部冲击等社会现象，与真实实验结果一致。

不足与发展：LLM 成本大、效率瓶颈，未来可融入多模态／异构 agent、行为学习等。

具体来说
这篇中的agent构成 分为心智mind 、  心智-行为耦合  、行为act

mind本身
每个智能体构建稳定的个体画像（如性格、年龄、性别）和动态的个人状态（如情感、经济状况和社会关系），
以确保智能体在不同情境下展现个性化的行为模式。
在此基础上，团队引入情感（Emotions）、需求（Needs）和认知（Cognition） 三大核心心理过程

耦合行为个人类比plan
智能体会优先满足生存和安全需求，在此基础上逐步追求社交等更高级目标。与此同时，基于计划行为理论，智能体在形成目标后，会结合自身认知和环境因素主动规划行动，使其行为既具适应性，又能展现长期连贯性。

行为act
简单行为不涉及复杂决策，但对智状态和长期行为模式至关重要。
复杂社会行为方面，智能体能够在移动、社交和经济活动中展现高度的自主智能。

二、真实社会环境
【个人认为核心在于具有的Env】
环境结合物理约束、社会规则和资源限制，确保智能体行为符合逻辑，避免幻觉  

实验：
实验模拟了 1,000、10,000、100,000 和 1,000,000 个智能体在城市、社交和经济空间内的交互，并采用典型工作日出行模式，在 8:30 早高峰启动模拟。
具体社会学实验：观点极化传播模拟
煽动性消息的传播模拟与智能治理
全民基本收入政策推演
飓风冲击下社会动态模拟

文章提供了简单的在线环境模拟和离线本地化仓库

```note
启发意义在于 这里有了明确的环境模块 但是依然是写死的 团队划分的是城市空间 社交空间 经济空间
架构启发意义【框架实现上采用了ray】
智能体提供可交互接口，支持真实决策

本文的叙事意义在于智能社会治理探索，
AgentSociety 将成为人机共生、治理创新、后稀缺经济模式的试验场，
测试 AI 议员参与立法对民主决策的影响，模拟 UBI 与机器人税组合政策，甚至在数字环境推演 AI 时代的法律与伦理框架，
探讨科技与社会的共存模式。
```

3. **A Survey on Social Simulation**
   - 论文连接：https://arxiv.org/abs/2412.03563
综述。

一句话总结：关于社会模拟方法和工具的最新综述，涵盖 agent‑based 模型与 LLM 驱动方法。

What：系统性梳理 agent‑based social simulation 的发展、方法与工具。

Why：社会科学与 AI 模拟方式融合日益加深，需要全景式认识研究路径。

技术动机：比较传统 ABM、规则模型与 generative LLM agent 方法。

How：总结已有文献分类、比较优缺、指出空白与趋势。

优势：视野广；为框架设计提供全面参考。

局限：仅综述，不提供系统实现；实时更新需要补充。

具体而言 目前的模拟
研究可按模拟粒度分为三类：
个体模拟 (Individual)、场景模拟 (Scenario)、社会模拟 (Society)。
三者在目标、系统复杂度、数据依赖与评估方式上逐级扩展

Individual Simulation（个体模拟）
核心目标
高保真复现特定人物或群体代表性行为，实现个性一致性、记忆连续性与情境化反应。
模块化架构（Profile / Memory / Planning / Action）
概要 (Profile)：年龄、性别、心理与背景特征；人工/模型生成；指导行为基调。
记忆 (Memory)：短期 vs 长期；写入 / 检索 / 反思；支持行为一致性。
规划 (Planning)：模拟人类决策思维；区分共情式（考虑他人）与主观式（以角色自我为中心）。
行为 (Action)：将内部决策转化为输出；环境形式（对话、情境）；行为开放域 vs 封闭域。
构建策略
非参数化提示：上下文注入角色/群体数据；快速、低成本。
参数化训练：预训练、微调、强化学习；提升稳健与一致性。
模拟对象尺度
人群个体：抽象群体原型，用于意见、偏好与偏见分析；多用提示式构建。
人物个体：具体角色（真实/虚拟）；可用更高质量定制数据。
评估方法
静态评估：问答/选择题/访谈；主观打分 vs 定量指标。
交互评估：在交互环境中测一致性、任务表现、角色扮演质量；多阶段+实时反馈。

Scenario Simulation（场景模拟）
核心思想
个体协作完成目标；需设计多智能体系统：环境、角色、组织、通信。
2.2 系统四要素
Environment / Role / Organization / Communication
Environment：
配置（初始设定、目标）
状态（动态感知）
历史（交互轨迹）
工具（外部调用如 Python/SQL 等，提高精度）
Role：
参与者 (task executors)
引导者 (planner / coordinator / integrator)
Organization：
模式（静态稳定 vs 动态重组）
结构（拓扑：集中式、分布式、分层等）
Communication：
形式（自然语言 vs 结构化协议）
风格（合作 vs 竞争）
2.3 场景类型
对话驱动：社交互动 / 问答 / 游戏；偏通用目标。
任务驱动：领域定制；含基础&应用科学、软件开发、行业任务；强调专业分工与工具使用。
2.4 场景级评估（Task / Subtask / System）
任务评估：整体成功、准确率、pass@k、成交价等；辅以 LLM 或人工定性评判。
子任务评估：分阶段表现（步骤数、重规划率、效率、代码完整性/一致性）；LLM 胜率或人工成本打分。
系统评估：效率、token 成本、人性化度、诊断类精度指标 (P/R/F1)；人工 Likert 补充质性观察。

Society Simulation（社会模拟）
3.1 研究重心
不以完成单一任务为目标，而是研究大规模交互带来的涌现行为与宏观社会模式。挑战在于桥接个体粒度与群体/社会尺度。
3.2 社会构建四要素
Composition / Network / Social Influence / Outcome
Composition（组成）：多样性（信仰、偏好、规范、价值观）；构造方式：虚拟合成、真实数据集、基于真实分布采样。
关键权衡：精度 vs 规模；重点个体（名人/意见领袖）细粒度建模；干预可优先作用于关键节点。
Network（网络）：决定信息/影响传播结构。
线下：模拟面对面；随机/预定义/算法估计关系；大规模时可抽象为社区统计。
线上：社交平台数据、抓取或合成；真实+合成混合、基于相似性连接。
Social Influence（社会影响）：因接收者特征与发出者地位不同而异；通过概要、记忆、认知偏差、规范机制建模；意见领袖与声誉是重要放大因子。
Outcome（结果）：集体态势≠个体线性叠加。
可度量宏观：投票、公众意见、统计指标随时间演化。
社会现象/规范：趋势观测、附加指标、案例分析（如规范形成、集体极化）。
社会级应用场景
广义经济学：资源分配、市场规则、激励机制、博弈、宏观趋势（失业、传染、资源流动）。
社会学与政治学: 民意估计、选举预测、危机公关、党派行为、组织行为、人格演变。
在线平台: 舆论传播、信息扩散、推荐系统交互、合成用户行为建模以评测算法。
社会级评估（Micro / Macro / System）
微观层级：单体行为拟人度、人类相似性指数、党派偏见对齐；与实证数据对比。
宏观层级：集体结果 vs 现实（传播规模、意见分布）；使用拟合度、相关系数等量化。
系统层级：扩展性、计算/资源成本、吞吐、效率。

演化趋势（时间线回顾）
注意：以下为研究发展“阶段性聚焦”的概括，不是严格分割。
4.1 个体模拟阶段
粗略模拟 (≈2022-06 起)：人格标签、知名角色扮演、表面特征对齐。
精细模拟 (≈2023-08)：认知一致性、心理特征建模、跨任务稳定性。
情境化模拟 (≈2024-04)：在具体场景中嵌入个体；情境影响行为差异。
4.2 场景模拟阶段
简单场景 (≈2023-01)：单一目标、基础互动。
多阶段场景 (≈2023-06)：顺序任务、阶段依赖、情境适应。
协作场景 (≈2024-02): 多智能体分工、协同计划、复杂任务管线。
4.3 社会模拟阶段
环境搭建期 (≈2023-06)：基本社交能力、记忆、对话、工具初探。
情境对齐期 (≈2024-02)：个性化建模、可测场景、提高模拟精度。
大规模验证期 (≈2024-02 以来持续)：扩展至大社会；验证社会规律（马太效应、帕累托等）；多模态（视觉/音频）增强现实感与行为丰富度。

```note
启发意义 目前正在写的MAS可能更像文章中个人+场景的角度 缺乏一些细节 更缺乏文中社会模拟的宏大组织
首先总结了 单agent一定要模块化架构（Profile / Memory / Planning / Action）
场景要求Environment / Role / Organization / Communication
文章认为社会要求Composition / Network / Social Influence / Outcome 强调涌现效应

最终文章展望的是
如何在精度/规模间自适应折衷？（分层建模、变量粒度缩放）
关键节点（意见领袖）如何识别与干预？（社会策略仿真）
社会网络不完备时如何真实+合成混合建模？
社会影响机制如何结合记忆、规范、认知偏差等心理变量？
多模态输入（视觉/音频）纳入后对行为一致性的影响与成本。
统一评测基准：跨层级指标映射（个体→场景→社会）。
```


4. **OASIS**
   - 论文连接：https://arxiv.org/pdf/2411.11581
   - Github：https://github.com/camel-ai/oasis

OASIS: Open Agent Social Interaction Simulations with One Million Agents

一句话总结：实现百万级 LLM 和规则混合 agent 仿真社交平台，模拟社交媒体行为与群体现象。

What：仿真 X / Reddit 社区用户行为，包括发帖、评论、关注、推荐系统互动。

Why：研究信息传播、极化、跟风效应等在线群体现象需要亿级用户规模。

技术动机：将规则 agent 与 LLM agent 结合、模拟动态社交网络与内容流。

How：支持 up to 一百万用户 agent，环境动态更新社交网络与帖子信息，同时 agent 执行动作与推荐决策。

优势：高扩展性、平台通用性强、能复现真实群体传播现象。

局限：基于社交媒体特化，不涵盖现实生活行为；LLM agent 成本依然较大。

具体而言：构建一个 可泛化（多平台可切换）、可扩展（百万级代理）、模块化（易替换/扩展） 的社交媒体模拟系统，用以研究信息传播、群体极化、从众/羊群效应、虚假信息扩散等社会现象。
截止到2024.11

| 系统         | 规模 (#Agents) | 环境/平台          | 行为维度      | 推荐系统  | 动态网络  | 主LLM资源 | 备注        |
|------------|--------------|----------------|-----------|-------|-------|--------|-----------|
| Smallville | 25           | 小镇             | -         | ×     | ×     | OpenAI | 早期社会场景模拟  |
| Sotopia    | 2            | -              | -         | ×     | ×     | OpenAI | 双人互动博弈类   |
| RecAgent   | 5            | -              | 6         | ✓     | ×     | OpenAI | 推荐场景      |
| Agent4Rec  | 1K           | 电影推荐           | 5         | ✓     | ×     | OpenAI | 推荐研究      |
| S3         | 1K           | X              | 4         | ×     | ×     | OpenAI | 社媒传播      |
| HiSim      | 300/700      | X              | 5         | ×     | ×     | OpenAI | 社媒交互      |
| AgentTorch | 8.4M\*       | -              | -         | ×     | ✓     | OpenAI | 原型/族群抽象   |
| AgentScope | 1M           | -              | -         | ×     | ×     | 开源     | 框架化       |
| **OASIS**  | **1M**       | **X & Reddit** | **21 动作** | **✓** | **✓** | 开源     | 模块化 + 大规模 |

模块化架构（五大核心组件）
OASIS = Environment Server + RecSys (Info Filter) + Agent Module + Time Engine + Scalable Inferencer

Environment Server
关系型数据库维护平台状态：用户、帖子、评论、用户关系、行为轨迹 (trace)、推荐缓存。
支持动态更新（新增用户、帖子、关注等）。

RecSys（信息过滤 / 内容分发）
X 模式：
In-network：关注网络内容按热度排序。
Out-of-network：基于兴趣匹配（TwHIN-BERT 向量相似度）、时效性、创作者粉丝量、广播效应。
可调 in/out 配额。
Reddit 模式：
基于公开“hot score”算法：结合赞踩差值、时间衰减（经典 log+sign+时间公式）排序，取前 k 推荐。

Agent Module
源于 CAMEL 思想；含 记忆模块（历史交互、帖子元数据、行动理由）与 行动模块。
21 种行动（注册、刷新、查看趋势、搜索、发帖、转帖、关注/取关/屏蔽、赞/踩/撤销、评论相关操作、无动作等）。
引入 Chain-of-Thought (CoT) 推理，提升行为解释性。

Time Engine
每个代理一个 24 维“小时级活跃概率向量”，基于真实或自定义历史行为。
概率驱动激活，避免全体同步爆发。
仿真步长：默认 3 分钟/步；支持线性时间映射以便 Reddit 热度排序需要精细时间戳。

Scalable Inferencer（可扩展推理体系）
分布式并发：代理、环境、推理服务解耦，异步消息管道。
GPU 资源调度器跨卡负载均衡。
多进程批推理以支撑百万级代理。


大规模用户生成
结合真实用户数据 + 模型化关系网络（保持无标度/稀疏结构特征）。
生成人口属性（年龄、性格、职业等简化独立维度）。
核心用户 (core) 与普通用户 (ordinary) 混合；按兴趣采样建边；以 ~0.2 概率跟随核心用户防止过密。

实验暂略【个人认为是带着场景和目的指标的探测 不够通用 或许只是举例子】

关键发现
Finding 1 – 信息传播：规模/宽度贴近真实，但传播深度偏浅。
模拟曲线与真实传播总体对齐（~30% 归一化 RMSE）。
深度偏小可能因推荐系统简化 & 用户画像粒度不足造成“中间层扩散能力”不足。

Finding 2 – 群体极化可复现；未设安全对齐（uncensored）模型更易走向极端。
随互动轮次推进，“保守”立场愈发极端。
未对齐模型更倾向使用绝对化语言（如“永远更好”）。

Finding 3 – Reddit 羊群效应：代理对负面初始信号更“跟风”；人类更具审慎纠偏。
Up-treated 情况下，人与代理表现接近。
Down-treated 时，代理更容易持续踩/不给赞；人类反而倾向回调，平均得分更高 → 显示人类批判性。

Finding 4 – 规模驱动多样性与“有用度”提升。
核心 196 用户在不同总体规模（196 → 10,196 → 100,196）下收到更广泛观点暴露。
多样性显著增加；依据 Safe-RLHF 准则评估，回复 helpfulness 随规模增长而提升。

Finding 5 – 反事实信息下的羊群效应呈“踩向聚合”并随规模增强。
小规模时差异不显；增至 10K+ 代理后，down-treated 曲线明显高于其他组。
不仅踩更多，评论里“不同意/纠错”信号增强，显示大群体会诱发集体纠偏或放大反应（视设定而定）。

研究启示：
社交系统研究需要“规模”，很多群体动态在小规模下不显现。
推荐策略、时间节律、模型对齐程度都会显著影响舆论演化。
代理行为偏差（如负向信号跟风）提示模型社会风险评估的必要性。

```note
这篇是很直接的社会模拟架构参考 主要专注社交媒体网络 实体性质不强 也并不通用
```

5. **YuLan-OneSim**
   - 论文链接：https://arxiv.org/pdf/2505.07581
   - Github：https://github.com/RUC-GSAI/YuLan-OneSim

一句话总结：提供“零代码”自然语言建模模拟场景平台，LLM agent 支持可演化、高度自定义的社会模拟。

What：支持用户用自然语言描述模拟场景，平台自动生成 simulation。

Why：降低社会模拟设置门槛，使非专业用户也能构建复杂社会场景。

技术动机：自然语言驱动情境设计、环境规则与 agent 行为。

How：自然语言输入 → scenario 构造器生成模拟结构 → LLM 驱动 agent 执行与演化。

优势：易用性高，无需编程；场景灵活可调。

缺点：可能缺少精确控制，复杂社会机制建模不足。

具体而言：
YuLan-OneSim是由中国人民大学高瓴人工智能学院团队开发的一款创新型社交模拟器
五个关键方面具有显著优势：
无代码场景构建：用户只需通过自然语言交互描述和优化模拟场景，系统会自动生成所有模拟代码，大幅降低对编程专业知识的需求。
全面默认场景库：系统实现了50个预定义的模拟场景，涵盖经济学、社会学、政治学、心理学、组织学、人口统计学、法学和传播学8个领域。
可进化模拟：系统能够接收外部反馈并自动微调基础LLM，显著提高模拟质量。
大规模模拟：通过开发完全响应式智能体框架和分布式模拟架构，系统可处理多达100,000个智能体，确保更稳定可靠的模拟结果。
AI社会科学研究员：基于上述功能，系统开发了AI社会科学研究员，用户只需提出研究主题，AI研究员就能自动分析输入、构建模拟环境、总结结果、生成技术报告并完成报告评审与优化。

四个核心子系统构成
- 场景自动构建
采用ODD协议标准化用户需求
构建行为图定义智能体交互逻辑
自动生成可执行代码
支持场景数据规范化和默认场景库
- 模拟子系统：
灵活的智能体和环境配置
响应式智能体框架
分布式模拟架构(主节点-工作节点)
实时监控和管理功能
- 反馈驱动进化子系统：
采用验证器-推理器-优化器-调谐器(VR2T)多智能体框架
通过系统或人类反馈改进模拟真实性
支持监督微调(SFT)和强化学习(RL)
- AI社会科学研究子系统：
实验设计模块(灵感生成、评估和ODD协议生成)
报告生成模块(数据分析、大纲撰写、报告生成和评审)

实验指标是
场景自动构建质量 模拟可靠性 模拟效率与可扩展性 AI社会科学研究员效果

已将项目完整开源(https://github.com/RUC-GSAI/YuLan-OneSim
```note
大规模场景  以及自然语言生成场景 有意思  
但是要仔细研究现存的多智能体框架

```
6. **LMAgent**
   - 论文连接：https://arxiv.org/pdf/2412.09237


LMAgent: A Large-scale Multimodal Agents Society for Multi-user Simulation

一句话总结：多模态 LLM agent 社会模拟平台，举例电商场景中 agent 可浏览、购买、直播、评论。

What：展示 agent 在电商环境中的聊天、购物、直播等行为。

Why：真实模拟人类在复杂多模态环境中的社交与消费行为。

技术动机：结合文字、图像、多模态 prompt chain‑of‑thought，提升 agent 决策一致性。

How：使用 self-consistency prompting、small‑world 模型＋ fast memory 机制减少 LLM 调用，支持 1 万级 agent。

优势：支持多模态输入输出、更真实环境模拟；系统效率提升约 40%。

局限：暂限于特定电商场景；扩展至泛社会模拟需定制多模态环境。

具体：
真实社会是多模态、动态、海量多用户交互系统；传统基于文本的小规模多Agent研究难以逼真模拟复杂社会行为，尤其在电商场景下（浏览-购买-评价-社交-直播带货等）。LMAgent旨在构建可扩展至万级（10,000+）多模态智能体社会，模拟电商用户在社交与消费场景中的可信行为，并观察群体现象（如从众行为）。

引入 自一致(Self-consistency)分阶段提示机制，显著提升多模态决策一致性与行为可信度，相比现有LLM多Agent系统表现更优。

提出 快速记忆(Fast Memory) + 记忆银行(Memory Bank) 与 小世界网络(Small-world) 初始化策略，使系统在支持万级智能体时依旧高效运行，Token成本下降约40%，且模拟出更真实的群体行为（如群购/从众）。

核心系统：（内外双层行为）
内部行为：人格(Persona)、记忆(含传感、短期、长期、遗忘机制)、规划(Planning)、反思(Reflection)。
外部行为：社交（聊天、发帖、好友互动、少量“明星”直播带货）、购物（浏览、搜索、翻页、查看详情、购买、评价），支持图像+文本多模态输入。
内外行为互相影响：外部交互写入记忆→驱动未来决策；反思与人格调节行为风格。

人格建模 (Persona)
为每个Agent随机生成：姓名、性别、年龄（截断正态分布）、职业、性格特质、偏好、行为倾向；LLM基于已知属性推断偏好，形成个性化消费与社交风格。

快速记忆机制 (Fast Memory)
为解决大规模系统的调用成本与延迟问题：

传感记忆 (Sensor)：记录当前观察 即时压缩为精炼句子
短期记忆 (ST Memory)：结构化存储 有重要性得分
长期记忆 (LT Memory)：相似记忆（向量相似度）引入时间新旧度与重要性驱动遗忘函数 符合神经科学
记忆银行 (Memory Bank)：对大量重复、低价值“基础行为”（如进商城、进社交区）缓存其类型、重要性、嵌入，避免重复LLM压缩；基础行为占>60%，带来约40%效率提升。

自一致提示机制 (Self-Consistency Prompting)
多模态决策分两阶段：

自我汇总阶段：基于个体特征 与最近观察 生成内部状态摘要 ，强化个体一致性。
环境融合阶段： 与环境多模态信息 （商品图文等）联合推理下一动作
该解耦可降低LLM单步负荷并提升动作合理度与一致性。

社会网络：小世界拓扑 (Small-world Topology)
初始：将N个节点按环状连接，每节点与最近 k/2邻居相连（局部聚类高）。
随后以概率p重连边→引入长程跳跃→缩短平均路径。
目标：兼具高聚类 + 短路径，贴近“六度空间理论”，提高消息扩散效率，模拟真实社交图谱。

沙盒仿真流程 (Algorithm 2 概要)
循环时间步：
若触发，Agent基于记忆与人格规划/反思；
经自一致提示选择下一动作（购物或社交）；
执行动作并更新记忆（含快速记忆处理）；
记录日志用于分析；
支持暂停观测或连续运行研究群体现象。

实验细节略

综合结论
LMAgent在多模态输入、行为一致性建模、快速记忆与小世界社交拓扑的共同作用下，成功实现 万级规模、行为可信、可演化 的电商用户社会模拟：
决策准确率显著优于传统推荐与多Agent基线；
对社会影响敏感，可复现直播带货等真实现象；
具有统计学意义的群体行为（从众、共购模式）与真实用户数据高度吻合；
高效运行（Token省约40%），具备面向更大规模与更复杂场景扩展的潜力。

启示
多模态 + 分阶段推理 可提升大规模社会模拟内在一致性。
层次化记忆+缓存 是控制大模型成本的关键工程化手段。
网络结构即社会假设：选择拓扑会显著改变扩散、互动与群体行为模式。
仿真产生的数据可用于研究推荐系统干预、社交影响建模、经济行为预测、应急舆情扩散等。
```note
聚焦购物场景 但是价值在于多模态和大规模的处理手段  文章的理论值很高
内外双层行为和是否要独立act的想法很像 

启发的是 这里把agent个人有关的小世界子图也自行存储 模拟更真实的行为
并且具有仿真的步长设置

```

7. **Light Society**
   - 论文连接：https://arxiv.org/pdf/2506.12078

一句话总结：实现地球级别十亿 agent 社会模拟平台，将社会过程抽象为结构化状态转换事件队列驱动。

What：地球规模、人类行为 agent 和环境状态的 agent‑based 模拟平台。

Why：研究全球性社会结构与大规模 emergent 行为。

技术动机：使用事件队列、模块化结构、LLM 操作支持极大规模模拟。

How：将社会进程规范化为 agent‑environment 状态转换，由 LLM 驱动并通过事件队列顺序执行。

优势：真正支持十亿级模拟，框架通用性强、性能高；可用于全球模拟研究。

局限：实际部署成本高，LLM 资源开销大；仍需验证行为真实性。

具体：
系统形式化定义 
Light Society 将社会过程形式化为由 智能体状态 与 环境状态 之间的结构化状态转移，并由 LLM 驱动的模拟算子 (Simulation Operations, SimOps) 在 事件队列 (Event Queue) 中调度执行。
【个人认为很有理论价值】
D：种子数据集 (seed dataset)，用于初始化。
T：时间轴。
S_A：智能体状态（详述见下）。
S_E：环境状态。
V：事件集合（agent ↔ agent / agent ↔ env 交互）。
Q：事件优先队列（按时间 & 优先级排序，保证时序一致性）。
F：模拟算子集合：

智能体状态 S_A
分三层：
静态画像（demographics / personality）——恒定。
内部状态（memory, beliefs, goals, emotion 等）——随时间演化。
外部状态（地理位置、社会关系、可观察行为等）。

环境状态 S_E
静态部分：地理布局、制度结构、物理约束等。
动态部分：天气、灾害、政策变化等时变因子。

3.3 事件 V
离散交互或状态改变（发消息、移动、交易、关系建立等）。
含时间戳、发起实体、目标实体、结构化负载（payload：文本/资源/意图）、优先级。
全部进入 事件队列 Q（优先队列），驱动模拟推进。

模拟算子 F（LLM 驱动）
Initialization
Perception
Policy / Decision  基于感知 + 内部状态 → 行为/事件生成
Agent Evolution  内在演化：记忆衰减、情绪漂移、动机变化等
Environment Evolution 环境随时间或事件改变
Update 将事件作用到系统，提交状态转移

模拟结束后执行 Readout：提取历史状态与事件，生成可分析输出（表格、图表、报告）。

How to Scale to >1B Agents
Light Society 通过多层策略显著降低 LLM 调用成本并提高吞吐：
4.1 语义提示缓存 (Semantic Prompt Caching)
对每次 LLM 查询生成向量嵌入存入向量数据库。
新提示检索近似语义相似项 → 可重用历史回答或模板，减少重复推理。
4.2 知识蒸馏 → 轻量代理模型 (Surrogate Models)
从实时/任务分布采样生成 LLM I/O 数据集。
训练轻量模型近似 LLM 行为（常规或高频决策）。
持续再训练以适应领域漂移，保持保真度。
4.3 多模型混合推理架构 (Mixture-of-Models, MoM)
自动负载均衡：全量 LLM、蒸馏模型、异构推理后端混合调度。
fidelity ↔ cost 可动态折衷。
4.4 并行化推理流水线
将模拟任务按事件/智能体分片，在异构硬件/模型间并行执行。
4.5 系统级分布式执行引擎
分布式状态管理；跨节点高效调度。
压缩图表示支撑十亿节点邻接查询与通信。
4.6 事件队列级优化
事件聚合：同时间同类型事件合并，减少冗余推理/更新。
异步执行：解耦独立事件，提高并发度与资源利用率。

实验：
信任博弈 (Trust Game) 模拟 目标：考察不同社会经济背景（社会阶层、教育、年龄等）下的信任与互惠行为
十亿级网络中的观点传播 (Opinion Propagation) 情境：围绕争议议题“AI 自动化将导致大规模失业”展开的社交影响过程；聚焦影响者 (influencer) → 被影响者 (influencee) 交互。


局限：
LLM 行为偏置会映射到社会模拟结果（模型间差异明显）。
人口属性来自 WVS；真实世界行为与问卷自报可能不一致。
蒸馏模型虽整体保真，但在长程因果链、罕见事件、群体突变上可能退化。
当前环境动态与多议题耦合仍有限；多尺度地理/制度/媒介传播层尚待扩展。
伦理：在政策仿真中如何避免将模型偏差误当真实社会规律？

未来工作方向
更丰富的记忆机制与跨回合学习（纵向行为建模）。
更真实的多模态环境输入（地理、媒体、经济指标）。
与真实世界时序数据接驳：政策、公共舆情、市场数据。
多议题互动、价值观集群、联盟/对抗结构。
高层指标到微观行为的反向约束（calibration / inverse modeling）。
面向政策评估、干预仿真、社会风险预警的应用化落地。



```note
感觉在事件的建模上很有启发
并且本文超大规模agent >1B

方法学上亮点：
状态数据结构
AgentID → {static, internal, external}
EnvState 分层：static map + dynamic overlays
事件结构：{ts, priority, initiators[], targets[], payload{...}}

推理调度策略
Prompt 模版 + slot 填充 + 嵌入去重
相似度阈值：high reuse / low fallback to LLM
MoM Router：cost budget / fidelity class /冷却时间 /缓存命中率

并行执行
分片：按图分区 / 地理分区 / 社会社群分簇
无依赖事件批处理；弱依赖异步提交；冲突检测 + 回滚

蒸馏循环
在线采样真实交互
聚类挑选代表性上下文
批量 LLM 标注 → 训练轻量模型
A/B 验证 → 动态路由切换阈值
```

8. 个人附加论文
2025.07 Multi-Actor Generative Artificial Intelligence as a Game Engine
一句话总结：
“把 TTRPG 的 Game Master 机制做成可插拔组件，用‘实体-组件’架构把 Concordia 升级成通用多智能体生成式 AI 引擎。”

what
• 提出 Evaluationist／Dramatist／Simulationist 三类使用动机，将 Concordia 重新设计为基于 Entity-Component 的可配置游戏引擎，让 GM、玩家、环境都变成可拼装组件。
• 提供并行、回合、异步三种调度引擎，支持从两人对话到万人经济的任意规模模拟。

why
• 单一系统难以同时满足评估、叙事、仿真三类需求；传统 ABM/RL 奖励模型过于简化，无法生成可信社会行为；需要可扩展、低代码、高复用的统一框架。

how
• 把“实体”当唯一 ID，把“组件”当可插拔功能模块（记忆、规划、叙事导演……）。
• 工程师写可复用组件 → 设计师用 YAML/JSON 拼装场景 → LLM 负责自然语言动作解析与世界状态更新。
• 支持 BYO-LLM，可接真实 API，数据生成反哺训练。

优势
• 高度模块化：同一套引擎可快速切换评估、讲故事或社会科学仿真。
• 低代码：设计师无需改源码即可组合/调参。
• 可扩展：组件级并行、异步执行，支持大规模并发。
• 复用性：组件和 Prefab 可在不同项目间共享。

局限
• 完全依赖外部 LLM，成本、延迟、幻觉问题不可控。
• 当前没有自动优化或元学习机制，复杂场景仍需人工调参。
• 缺乏统一基准，不同动机下的结果难以横向比较。
• 法律与伦理风险：开放世界生成可能产生不当内容，需额外护栏。
```note
目前最新的一篇文章
由于和个人几天前的想法不谋而合
有共鸣的快乐和被抢先的痛苦

可以借鉴的核心：
1 语言模型集成层 统一的 LanguageModel 抽象接口，支持多种 LLM 提供商
2 记忆系统 AssociativeMemory 提供基于语义搜索的上下文检索能力
3 核心组件只有三种 
ContextComponent: 提供行动和观察的上下文信息
ActingComponent: 决定实体的具体行动 entity_component.py:298-321
ContextProcessorComponent: 处理其他组件的上下文 entity_component.py:324-357
4 更高一层的引擎层 (Engine Layer)
Sequential/Simultaneous Engine: 控制仿真执行模式
Prefab System: 提供可配置的实体创建模板

目前实验室提出的MAS框架在个人看来就可以改成文章中的EC引擎架构 非常灵活自由
但是文章缺少回滚操作  如果本框架可以统一管理数据操作和影响范围 就可以实现更强大的因果时间模拟

PS如何快速分析github公开仓库的代码结构等：替换GitHub URL中的github为deepwiki
```


## 实践手册

### Multi-Agent框架：

#### LangGraph
基于LangChain打造的Multi-Agent框架，通过引入有向循环图的理念，打造了一个极具灵活性和可定制性的解决方案。LangGraph不仅适用于各类Multi-Agent任务，还能支持几乎所有的多智能体编排应用，使其成为那些面临复杂任务、追求高度灵活性和定制化能力的开发者的首选工具。

- 使用教程：https://langchain-ai.github.io/langgraph/#get-started
- GitHub：https://github.com/langchain-ai/langgraph



#### docker

**docker学习资料（可选）**

课程视频（可以学习一些操作命令）：
- https://www.bilibili.com/video/BV1Zn4y1X7AZ/?share_source=copy_web

**安装docker**
- 操作文档：https://help.aliyun.com/zh/ecs/use-cases/install-and-use-docker#8dca4cfa3dn0e

**docker 国内镜像配置**
- Linux操作文档：https://blog.csdn.net/Lichen0196/article/details/137355517
- MacOS操作文档：https://blog.csdn.net/u011308433/article/details/132058397

**vscode 连接docker 容器**
- 操作文档：https://blog.csdn.net/qq_19716143/article/details/132310200

**docker 命令**

镜像操作：
- 检索：`docker search <镜像名>`
- 下载：`docker pull <镜像名>`
- 列表：`docker images`
- 删除：`docker rmi <镜像名:版本/镜像ID>`

容器命令：
- 运行：`docker run <容器名/ID>`
- 查看：`docker ps`
- 停止：`docker stop <容器名/ID>`
- 启动：`docker start <容器名/ID>`
- 重启：`docker restart <容器名/ID>`
- 状态：`docker stats <容器名/ID>`
- 日志：`docker logs <容器名/ID>`
- 进入：`docker exec -it <容器ID> /bin/bash`
- 删除：`docker rm (-f 强制删除运行中的) <容器名/ID>`
- 停止所有运行的容器：`docker stop $(docker ps -q)`
- 删除所有容器：`docker rm $(docker ps -a -q)`

**docker run 详解（docker run <参数> <容器名/ID>)**
- `-d` 在后台运行
- `--name` 容器名字
- `-p （本机端口号）:（容器端口号）` 端口映射

**保存镜像 （从容器中）**
- 提交：`docker commit`
- 保存：`docker save`
- 加载：`docker load`

**分享社区**
- 登录：`docker login`
- 命名：`docker tag`
- 推送：`docker push`

**构建镜像 （从dockerfile中）**
```bash
docker build [OPTIONS] <构建上下文路径>
```

常用选项：
- `-t <镜像名:标签>`：指定镜像名称和标签（如 -t my-app:v1）。
- `-f <Dockerfile路径>`：指定非默认名称的 Dockerfile（如 -f ./path/to/Dockerfile）。

构建上下文路径：
通常是包含 Dockerfile 的目录（如 . 表示当前目录）。
Docker 会将此目录下的文件发送给守护进程（需注意排除无关文件，通过 .dockerignore 配置）。

**管理多容器（与k8s类似，可在单机上开发使用）**
```bash
# 启动所有容器服务
docker compose -f <yaml文件路径> up -d (后台启动)

# 停止并删除所有容器、网络
docker compose -f <yaml文件路径> down 
-v：同时删除匿名卷  --rmi all：删除所有相关镜像

# 修改 Dockerfile 后需要完全停止并删除现有容器，以确保新镜像被正确应用
docker compose down

# 使用修改后的 Dockerfile 重新构建镜像，并启动新的容器
docker compose up -d --build
```


#### k8s
- 课程视频：https://www.bilibili.com/video/BV1Se411r7vY/?share_source=copy_web&vd_source=c54b306489f493ab850de370eaa4b0c5
- 通过kubectl工具连接集群：https://help.aliyun.com/zh/ack/ack-managed-and-ack-dedicated/user-guide/obtain-the-kubeconfig-file-of-a-cluster-and-use-kubectl-to-connect-to-the-cluster?spm=a2c4g.11186623.0.0.613ad176I3BtlO#section-2za-tyw-p71 